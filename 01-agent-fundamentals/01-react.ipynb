{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct Agents from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will be build a ReAct agent *from scratch*. That is, no frameworks or unecessary abstractions so that we can truly understand how ReAct agents work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be modifying ReAct a little to take advantage of the progress that has been made in LLMs since the paper was originally released. We make two modifications:\n",
    "\n",
    "1. We use chat models, when ReAct was released LLMs were not fine-tuned specifically for chat and instead were prompted to generate chat-like dialogues. Most **S**tate **o**f **t**he **A**rt (SotA) LLMs nowadays are built specifically for chat and so the input into them must be modified to be chat-model friendly.\n",
    "\n",
    "2. We will use JSON-mode to force structured output from our LLMs.  The original ReAct method simply instructed the LLM to output everything in a particular format. That works but is prone to occasionally breaking. By forcing JSON-like output we reduce the likelihood of poorly structured output *and* make it easier for our downstream code to parse and use the output from our LLM. To accomodate this we modify the instructions to ask for `thought` and `action` steps in a JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant. Given a user query you must provide a `thought` and\n",
    "`action` step that take one step towards solving the user's query. Both the\n",
    "`thought` and `action` steps will be contained in JSON output.\n",
    "\n",
    "The `thought` is the first key mapping to your reasoning on how to solve the\n",
    "user's query.\n",
    "\n",
    "The `action` step is the second key mapping that describes how you wish to use\n",
    "the chosen tool to solve the user's query. It contains a `tool` key mapping to\n",
    "the name of the tool to use and a `args` key containing a JSON object of\n",
    "arguments to pass to the tool.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "user: What is the weather in Tokyo?\n",
    "assistant: {\n",
    "  \"thought\": \"I need to find out the current temperature in Tokyo\",\n",
    "  \"action\": {\"tool\": \"search\", \"args\": {\"query\": \"current temperature in Tokyo\"}}\n",
    "}\n",
    "\n",
    "If you have performed any previous thought and action steps, you will find them\n",
    "below under the \"Previous Steps\" section. Alongside these you will find an\n",
    "`observation` key containing the output of those previous actions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't defined any tools or agent logic yet, but let's see what type of output\n",
    "our LLM produces if we prompt it with this system prompt.\n",
    "\n",
    "Make sure you have Ollama running and Llama 3.2 downloaded by executing this in your terminal:\n",
    "\n",
    "```\n",
    "ollama pull llama3.2:3b-instruct-fp16\n",
    "```\n",
    "\n",
    "If you need guidance on setting up Ollama, please refer to our [guidelines](https://github.com/aurelio-labs/agents-course?tab=readme-ov-file#ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"thought\": \"I need to determine the current date\",\n",
      "  \"action\": {\n",
      "    \"tool\": \"date\",\n",
      "    \"args\": {}\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "res = ollama.chat(\n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"What is the date today?\"},\n",
    "    ],\n",
    "    format=\"json\",\n",
    "    options={\"temperature\": 0.0}\n",
    ")\n",
    "\n",
    "print(res[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we're outputting the correct format *however* we don't have a `date` tool, in fact, we don't have *any* tools! Let's define some.\n",
    "\n",
    "First, we'll define a search tool. This tool will allow our agent to search the web for information. To implement it we will use the Tavily API, it comes with a number of requests for free but we do need to [sign up for the API](https://app.tavily.com/home) and get an API key to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the weather in Tokyo?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'title': 'Weather in Tokyo',\n",
       "   'url': 'https://www.weatherapi.com/',\n",
       "   'content': \"{'location': {'name': 'Tokyo', 'region': 'Tokyo', 'country': 'Japan', 'lat': 35.6895, 'lon': 139.6917, 'tz_id': 'Asia/Tokyo', 'localtime_epoch': 1730676560, 'localtime': '2024-11-04 08:29'}, 'current': {'last_updated_epoch': 1730675700, 'last_updated': '2024-11-04 08:15', 'temp_c': 17.9, 'temp_f': 64.3, 'is_day': 1, 'condition': {'text': 'Partly Cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 5.6, 'wind_kph': 9.0, 'wind_degree': 181, 'wind_dir': 'S', 'pressure_mb': 1023.0, 'pressure_in': 30.22, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 61, 'cloud': 60, 'feelslike_c': 17.9, 'feelslike_f': 64.3, 'windchill_c': 17.9, 'windchill_f': 64.3, 'heatindex_c': 17.9, 'heatindex_f': 64.3, 'dewpoint_c': 10.5, 'dewpoint_f': 50.8, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 0.7, 'gust_mph': 7.4, 'gust_kph': 12.0}}\",\n",
       "   'score': 0.9998726,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Tokyo weather forecast, rainfall, air quality, severe weather warning ...',\n",
       "   'url': 'https://www.qweather.com/en/weather/tokyo-65E77.html',\n",
       "   'content': 'Tokyo real-time weather and 30 days forecast, also include air quality, precipitation, severe weather warning. Tokyo real-time weather and 30 days forecast, also include air quality, precipitation, severe weather warning. Forecast ... 2024-11-02 03:21. 18°',\n",
       "   'score': 0.9990264,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Tokyo, Tokyo, Japan Weather Forecast | AccuWeather',\n",
       "   'url': 'https://www.accuweather.com/en/jp/tokyo/226396/weather-forecast/226396',\n",
       "   'content': 'Get the latest weather forecast for Tokyo, Japan, with hourly, daily, and 15-day details. Check the temperature, humidity, wind, rain, and snow from AccuWeather.',\n",
       "   'score': 0.9380106,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Tokyo, Tokyo, Japan Daily Weather | AccuWeather',\n",
       "   'url': 'https://www.accuweather.com/en/jp/tokyo/226396/daily-weather-forecast/226396',\n",
       "   'content': \"Know what's coming with AccuWeather's extended daily forecasts for Tokyo, Tokyo, Japan. Up to 90 days of daily highs, lows, and precipitation chances.\",\n",
       "   'score': 0.86511016,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Tokyo, Tokyo, Japan Monthly Weather | AccuWeather',\n",
       "   'url': 'https://www.accuweather.com/en/jp/tokyo/226396/november-weather/226396',\n",
       "   'content': 'Get the monthly weather forecast for Tokyo, Tokyo, Japan, including daily high/low, historical averages, to help you plan ahead.',\n",
       "   'score': 0.8330532,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 3.19}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "TAVILY_API_KEY = \"tvly-...\"  # put your API key here!\n",
    "\n",
    "tavily_url = \"https://api.tavily.com\"\n",
    "\n",
    "res = requests.post(\n",
    "    f\"{tavily_url}/search\",\n",
    "    json={\n",
    "        \"api_key\": TAVILY_API_KEY,\n",
    "        \"query\": \"What is the weather in Tokyo?\"\n",
    "    },\n",
    ")\n",
    "\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we don't return much useful info here beyond that we have some URLs that *do contain* the information we need. Fortunately, we can extract this information via Tavily's `/extract` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [x[\"url\"] for x in res.json()[\"results\"]]\n",
    "\n",
    "res = requests.post(\n",
    "    f\"{tavily_url}/extract\",\n",
    "    json={\n",
    "        \"api_key\": TAVILY_API_KEY,\n",
    "        \"urls\": urls\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'url': 'https://www.qweather.com/en/weather/tokyo-65E77.html',\n",
       "   'raw_content': 'Japan\\xa0\\xa0\\xa02024-11-04\\xa0\\xa0\\xa0Monday\\xa0\\xa0\\xa035.68N, 139.81E\\nTokyo\\nTokyo\\n2024-11-04 07:52\\n12°\\nSunny\\n\\n                    3KM/H\\n                \\nNNE\\n82%\\nHumidity\\xa0\\nModerate\\nUV\\xa0\\n12°\\nFeels Like\\n18km\\nVisibility\\n0.0mm\\nPrecipitation\\n1023hPa\\nPressure\\n24H Forecast\\nTemperature\\nForecast\\nToday\\n4 Nov\\nTue\\n5 Nov\\nWed\\n6 Nov\\nThu\\n7 Nov\\nFri\\n8 Nov\\nSat\\n9 Nov\\nSun\\n10 Nov\\nSun\\nNow\\n08:29\\nNow\\n08:29\\nAltitude\\n24°\\nHeading\\n133°SE\\nMoon\\nNow\\n08:29\\nNow\\n08:29\\nAltitude\\n-1°\\nHeading\\n122°ESE\\nTemperature\\nSatellite imagery\\nWeather Rank\\nAza Chatan\\nGinowan\\nGushikawa\\nKatsuren Haebaru\\nItoman\\nOkinawa-shi\\nTomigusuku-shi\\nIoto\\nIkenosawa\\nNago-shi\\nTeine-ku\\nOtaru-shi\\nYoichi-cho\\nAogashima\\nMombetsu-shi\\nKita-ku\\nHigashi-ku\\nNishi-ku\\nMinami-ku\\nMinami5-Jonishi\\nNearby\\nUrayasu-shi\\nShinagawa-ku\\nIchikawa-shi\\nMatsudo\\nYashio\\nNerima\\nSoka-shi\\nFunabashi\\nToda-shi\\nSato\\nQWeather APP\\nVisualize Your Weather\\nWeather API/SDK\\nNeed weather data service?\\nNEED WEATHER DATA ?\\nGet APP\\nForecast\\nAir Quality\\nSevere Weather\\nSatellite+Radar\\nTraffic Weather\\nVisualization\\nWeather Data\\nWeather API/SDK\\nWidget\\nOpenWeatherPlus\\nGithub\\nAbout\\nSupport\\nBlog\\nContact\\nQWeather APP\\nVisualization weather APP\\n© 2024 qweather.com All rights reserved\\n'},\n",
       "  {'url': 'https://www.accuweather.com/en/jp/tokyo/226396/november-weather/226396',\n",
       "   'raw_content': 'Tokyo, Tokyo, Japan Monthly Weather | AccuWeather\\nGo Back\\nTokyo, Tokyo ============ 60°F\\nLocation \\nLocation News Videos\\nUse Current Location\\nRecent\\nTokyo Tokyo 60°\\nNo results found.\\nTry searching for a city, zip code or point of interest.\\n\\nsettings\\nTokyo, Tokyo Weather\\nToday WinterCast Local {stormName} Tracker Hourly Daily Radar MinuteCast Monthly Air Quality Health & Activities\\nAround the Globe\\n\\n### Hurricane Tracker### Severe Weather### Radar & Maps\\nNews\\n### News & Features### Astronomy### Business### Climate### Health### Recreation### Sports### Travel\\n### Video\\nToday Hourly Daily Radar MinuteCastMonthly -------Air Quality Health & Activities For Business\\nNovember\\nJanuary February March April May June July August September October November December\\n2024\\n2023 2024 2025\\nDaily -----\\nS\\nM\\nT\\nW\\nT\\nF\\nS\\n27\\n75°\\n62°\\n28\\n69°\\n63°\\n29\\n66°\\n54°\\n30\\n72°\\n54°\\n31\\n68°\\n55°\\n1\\n71°\\n54°\\n2\\n62°\\n59°\\n3\\nN/A\\n4 75° 58°5 70° 55°6 60° 50°7 65° 47°8 61° 48°9 61° 53°10 61° 60°11 71° 58°12 72° 54°13 65° 50°14 64° 50°15 63° 48°16 62° 53°17 66° 52°18 66° 47°19 58° 44°20 57° 44°21 62° 51°22 64° 52°23 66° 50°24 61° 46°25 56° 48°26 58° 49°27 63° 48°28 59° 46°29 56° 47°30 56° 47°\\nTemperature Graph\\n°F\\nAvg. Hi\\nAvg. Lo\\nActual Hi\\nActual Lo\\nForecast Hi\\nForecast Lo\\n*Temporarily Unavailable\\nFurther Ahead\\n\\n### December 2024### January 2025### February 2025\\nAround the Globe\\n\\n### Hurricane Tracker### Severe Weather### Radar & Maps### News### Video\\nTop Stories\\nHurricane Tropical storm likely to brew in Caribbean; May reach US 13 minutes ago Severe Weather At least 11 injured after tornado-spawning thunderstorms hit Oklahoma 57 minutes ago Weather Forecasts Temperature roller coaster expected for Northeast, drought to continue 26 minutes ago Severe Weather Severe storms, flooding to impact central US through Election Day 3 hours ago Astronomy November touts 3 meteor showers, final supermoon of 2024 3 days ago  More Stories\\nFeatured Stories\\nWeather News Parts of Spain appear to merge with the sea after historic flood 2 days ago Weather News Daylight saving time 2024: When do clocks fall back? 2 days ago Astronomy Rover captures peculiar ‘googly eye’ in the Martian sky 2 days ago Weather News Spain hit by deadliest floods in decades 3 days ago Weather News CDC confirms onions caused McDonald\\'s E. coli outbreak 3 days ago \\nAround The Web\\nWorld Asia Japan Tokyo Tokyo\\nWeather Near Tokyo:\\n\\nFunabashi-shi, Chiba\\nHachioji-shi, Tokyo\\nKawasaki-shi, Kanagawa\\n\\nCompany\\nProven Superior Accuracy About AccuWeather Digital Advertising Careers Press Contact Us\\nProducts & Services\\nFor Business For Partners For Advertising AccuWeather APIs AccuWeather Connect RealFeel® and RealFeel Shade™ Personal Weather Stations\\nApps & Downloads\\niPhone App Android App See all Apps & Downloads\\nSubscription Services\\nAccuWeather Premium AccuWeather Professional\\nMore\\nAccuWeather Ready Business Health Hurricane Leisure and Recreation Severe Weather Space and Astronomy Sports Travel Weather News\\n\\nCompany\\nProven Superior Accuracy About AccuWeather Digital Advertising Careers Press Contact Us\\n\\nProducts & Services\\nFor Business For Partners For Advertising AccuWeather APIs AccuWeather Connect RealFeel® and RealFeel Shade™ Personal Weather Stations\\nApps & Downloads\\niPhone App Android App See all Apps & Downloads\\nSubscription Services\\nAccuWeather Premium AccuWeather Professional\\nMore\\nAccuWeather Ready Business Health Hurricane Leisure and Recreation Severe Weather Space and Astronomy Sports Travel Weather News\\n© 2024 AccuWeather, Inc. \"AccuWeather\" and sun design are registered trademarks of AccuWeather, Inc. All Rights Reserved.\\nTerms of Use | Privacy Policy | Cookie Policy | Do Not Sell or Share My Personal Information  Confirmed Not Selling Your Data\\nWe have updated our Privacy Policy and Cookie Policy.\\nI Understand\\nGet AccuWeather alerts as they happen with our browser notifications.\\nEnable Notifications\\nNo, Thanks\\nNotifications Enabled\\nThanks! We’ll keep you informed.\\n \\n'}],\n",
       " 'failed_results': [{'url': 'https://www.weatherapi.com/',\n",
       "   'error': 'Failed to get content'},\n",
       "  {'url': 'https://www.accuweather.com/en/jp/tokyo/226396/weather-forecast/226396',\n",
       "   'error': 'Request timed out'},\n",
       "  {'url': 'https://www.accuweather.com/en/jp/tokyo/226396/daily-weather-forecast/226396',\n",
       "   'error': 'Request timed out'}],\n",
       " 'response_time': 7.13}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most requests didn't work, but that's okay we made multiple requests and fortunately received one result that looks perfect, we can extract that information like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan   2024-11-04   Monday   35.68N, 139.81E\n",
      "Tokyo\n",
      "Tokyo\n",
      "2024-11-04 07:52\n",
      "12°\n",
      "Sunny\n",
      "\n",
      "                    3KM/H\n",
      "                \n",
      "NNE\n",
      "82%\n",
      "Humidity \n",
      "Moderate\n",
      "UV \n",
      "12°\n",
      "Feels Like\n",
      "18km\n",
      "Visibility\n",
      "0.0mm\n",
      "Precipitation\n",
      "1023hPa\n",
      "Pressure\n",
      "24H Forecast\n",
      "Temperature\n",
      "Forecast\n",
      "Today\n",
      "4 Nov\n",
      "Tue\n",
      "5 Nov\n",
      "Wed\n",
      "6 Nov\n",
      "Thu\n",
      "7 Nov\n",
      "Fri\n",
      "8 Nov\n",
      "Sat\n",
      "9 Nov\n",
      "Sun\n",
      "10 Nov\n",
      "Sun\n",
      "Now\n",
      "08:29\n",
      "Now\n",
      "08:29\n",
      "Altitude\n",
      "24°\n",
      "Heading\n",
      "133°SE\n",
      "Moon\n",
      "Now\n",
      "08:29\n",
      "Now\n",
      "08:29\n",
      "Altitude\n",
      "-1°\n",
      "Heading\n",
      "122°ESE\n",
      "Temperature\n",
      "Satellite imagery\n",
      "Weather Rank\n",
      "Aza Chatan\n",
      "Ginowan\n",
      "Gushikawa\n",
      "Katsuren Haebaru\n",
      "Itoman\n",
      "Okinawa-shi\n",
      "Tomigusuku-shi\n",
      "Ioto\n",
      "Ikenosawa\n",
      "Nago-shi\n",
      "Teine-ku\n",
      "Otaru-shi\n",
      "Yoichi-cho\n",
      "Aogashima\n",
      "Mombetsu-shi\n",
      "Kita-ku\n",
      "Higashi-ku\n",
      "Nishi-ku\n",
      "Minami-ku\n",
      "Minami5-Jonishi\n",
      "Nearby\n",
      "Urayasu-shi\n",
      "Shinagawa-ku\n",
      "Ichikawa-shi\n",
      "Matsudo\n",
      "Yashio\n",
      "Nerima\n",
      "Soka-shi\n",
      "Funabashi\n",
      "Toda-shi\n",
      "Sato\n",
      "QWeather APP\n",
      "Visualize Your Weather\n",
      "Weather API/SDK\n",
      "Need weather data service?\n",
      "NEED WEATHER DATA ?\n",
      "Get APP\n",
      "Forecast\n",
      "Air Quality\n",
      "Severe Weather\n",
      "Satellite+Radar\n",
      "Traffic Weather\n",
      "Visualization\n",
      "Weather Data\n",
      "Weather API/SDK\n",
      "Widget\n",
      "OpenWeatherPlus\n",
      "Github\n",
      "About\n",
      "Support\n",
      "Blog\n",
      "Contact\n",
      "QWeather APP\n",
      "Visualization weather APP\n",
      "© 2024 qweather.com All rights reserved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res.json()[\"results\"][0][\"raw_content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so this is how we use the Tavily API to search the web, now let's implement this logic within a function which we can then use as a tool (ie action) for our ReAct agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str):\n",
    "    \"\"\"Use this tool to search the web for information.\"\"\"\n",
    "    # first we need to search the web for the query\n",
    "    res = requests.post(\n",
    "        f\"{tavily_url}/search\",\n",
    "        json={\n",
    "            \"api_key\": TAVILY_API_KEY,\n",
    "            \"query\": query\n",
    "        },\n",
    "    )\n",
    "    # now get all the URLs from the search results\n",
    "    urls = [x[\"url\"] for x in res.json()[\"results\"]]\n",
    "    # now extract the information from the URLs\n",
    "    res = requests.post(\n",
    "        f\"{tavily_url}/extract\",\n",
    "        json={\n",
    "            \"api_key\": TAVILY_API_KEY,\n",
    "            \"urls\": urls\n",
    "        },\n",
    "    )\n",
    "    # we return just the top result as otherwise we overload our LLM\n",
    "    return res.json()[\"results\"][0][\"raw_content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yahoo Weather\n",
      "My Locations\n",
      "Around the World\n",
      "Tokyo\n",
      "Japan\n",
      "Mostly Sunny\n",
      "Forecast\n",
      "5 PM\n",
      "6 PM\n",
      "7 PM\n",
      "8 PM\n",
      "9 PM\n",
      "10 PM\n",
      "11 PM\n",
      "12 AM\n",
      "1 AM\n",
      "2 AM\n",
      "3 AM\n",
      "4 AM\n",
      "5 AM\n",
      "6 AM\n",
      "7 AM\n",
      "8 AM\n",
      "9 AM\n",
      "10 AM\n",
      "11 AM\n",
      "12 PM\n",
      "1 PM\n",
      "2 PM\n",
      "3 PM\n",
      "4 PM\n",
      "Clear with a high of 42 °F (5.6 °C) and a 49% chance of precipitation. Winds NW at 24 mph (38.6 kph).\n",
      "Night - Clear with a 28% chance of precipitation. Winds variable at 7 to 25 mph (11.3 to 40.2 kph). The overnight low will be 34 °F (1.1 °C).\n",
      "Sunny today with a high of 54 °F (12.2 °C) and a low of 32 °F (0 °C).\n",
      "Mostly cloudy today with a high of 56 °F (13.3 °C) and a low of 40 °F (4.4 °C).\n",
      "Rain today with a high of 52 °F (11.1 °C) and a low of 40 °F (4.4 °C). There is a 66% chance of precipitation.\n",
      "Rain today with a high of 47 °F (8.3 °C) and a low of 41 °F (5 °C). There is a 77% chance of precipitation.\n",
      "Showers today with a high of 48 °F (8.9 °C) and a low of 42 °F (5.6 °C). There is a 75% chance of precipitation.\n",
      "Mostly cloudy today with a high of 54 °F (12.2 °C) and a low of 39 °F (3.9 °C). There is a 51% chance of precipitation.\n",
      "Showers today with a high of 48 °F (8.9 °C) and a low of 34 °F (1.1 °C). There is a 51% chance of precipitation.\n",
      "Mostly sunny today with a high of 44 °F (6.7 °C) and a low of 32 °F (0 °C).\n",
      "Sunny today with a high of 51 °F (10.6 °C) and a low of 35 °F (1.7 °C).\n",
      "Sunny today with a high of 52 °F (11.1 °C) and a low of 37 °F (2.8 °C).\n",
      "Precipitation\n",
      "Wind & Pressure\n",
      "Details\n",
      "Today - Clear with a high of 42 °F (5.6 °C) and a 49% chance of precipitation. Winds NW at 24 mph (38.6 kph).\n",
      "Tonight - Clear with a 28% chance of precipitation. Winds variable at 7 to 25 mph (11.3 to 40.2 kph). The overnight low will be 34 °F (1.1 °C).\n",
      "Sun & Moon\n",
      "Map\n"
     ]
    }
   ],
   "source": [
    "print(search(query=\"What is the weather in Tokyo?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay that is our `search` tool. We will also define a tool that will be triggered when our LLM would like to provide it's final `answer` to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(answer: str):\n",
    "    \"\"\"Use this tool to provide your final answer to the user.\"\"\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate an additional part to our `system_prompt` to explain which tools are available to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'search',\n",
       "  'description': 'Use this tool to search the web for information.',\n",
       "  'args': {'query': 'str'}},\n",
       " {'name': 'answer',\n",
       "  'description': 'Use this tool to provide your final answer to the user.',\n",
       "  'args': {'answer': 'str'}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "# we get the various parameters/description from each tool function\n",
    "tools = [search, answer]\n",
    "tool_descriptions = [\n",
    "    {\n",
    "        \"name\": tool.__name__,\n",
    "        \"description\": str(inspect.getdoc(tool)),\n",
    "        \"args\": {\n",
    "            k: str(v).split(\": \")[1] for k, v in inspect.signature(tool).parameters.items()\n",
    "        }\n",
    "    }\n",
    "    for tool in tools\n",
    "]\n",
    "tool_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's parse these into text instructions that can be added to our `system_prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have access to the following tools ONLY, no other tools exist:\n",
      "\n",
      "{'name': 'search', 'description': 'Use this tool to search the web for information.', 'args': {'query': 'str'}}\n",
      "{'name': 'answer', 'description': 'Use this tool to provide your final answer to the user.', 'args': {'answer': 'str'}}\n"
     ]
    }
   ],
   "source": [
    "tool_instructions = (\n",
    "    \"You have access to the following tools ONLY, no other tools exist:\\n\\n\"\n",
    "    + \"\\n\".join([str(x) for x in tool_descriptions])\n",
    ")\n",
    "print(tool_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try calling our LLM again with these additional instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"thought\": \"I need to find out what Ollama refers to in the context of AI\",\n",
      "  \"action\": {\n",
      "    \"tool\": \"search\",\n",
      "    \"args\": {\"query\": \"Ollama AI\"}\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "res = ollama.chat(\n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"{system_prompt}\\n\\n{tool_instructions}\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Ollama in the context of AI?\"},\n",
    "    ],\n",
    "    format=\"json\",\n",
    "    options={\"temperature\": 0.0}\n",
    ")\n",
    "\n",
    "step = res[\"message\"][\"content\"]\n",
    "print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Our LLM has correctly generated the query we need. We can now parse this and pass it into the `search` tool as specified by our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Language Models Locally with Ollama: A Comprehensive Guide\n",
      "\n",
      "\n",
      "Follow\n",
      "\n",
      "Follow\n",
      "\n",
      "\n",
      "Run Language Models Locally with Ollama: A Comprehensive Guide\n",
      "\n",
      "Spheron Network\n",
      "·Nov 3, 2024·7 min read\n",
      "Table of contents\n",
      "\n",
      "Integration with LangChain\n",
      "Building a Simple Chatbot\n",
      "Using AnythingLLM with Ollama\n",
      "Best tools available, but unheard\n",
      "1. Haystack by Deepset\n",
      "2. LlamaIndex (formerly GPT Index)\n",
      "3. Chroma\n",
      "4. Hugging Face Transformers\n",
      "5. Pinecone\n",
      "6. OpenAI API\n",
      "7. Rasa\n",
      "8. Cohere\n",
      "9. Vercel AI SDK\n",
      "\n",
      "\n",
      "Conclusion\n",
      "\n",
      "Ollama is an open-source platform that simplifies the process of setting up and running large language models (LLMs) on your local machine. With Ollama, you can easily download, install, and interact with LLMs without the usual complexities.\n",
      "To get started, you can download Ollama from here. Once installed, open a terminal and type:\n",
      "ollama run phi3\n",
      "OR\n",
      "ollama pull phi3\n",
      "ollama run phi3\n",
      "This will download the required layers of the model \"phi3\". After the model is loaded, Ollama enters a REPL (Read-Eval-Print Loop), which is an interactive environment where you can input commands and see immediate results.\n",
      "To explore the available commands within the REPL, type:\n",
      "/?\n",
      "This will show you a list of commands you can use. For example, to exit the REPL, type /bye. You can also display the models you have installed using:\n",
      "ollama ls\n",
      "If you need to remove any model, use:\n",
      "ollama rm\n",
      "For a complete list of available models in Ollama, you can visit their model library, which contains details about model sizes, parameters, and more. Additionally, Ollama has specific hardware requirements. For instance, to run a 7B model, you'll need at least 8 GB of RAM; 16 GB for a 13B model, and 32 GB for a 33B model. If you have a GPU, Ollama supports it—more details can be found on their GitHub page. However, if you're running on a CPU, expect it to perform slower.\n",
      "Ollama also allows you to set a custom system prompt. For example, to instruct the system to explain concepts at a basic level, you can use:\n",
      "/set system Explain concepts as if you are talking to a primary school student.\n",
      "You can then save and reuse this setup by giving it a name:\n",
      "/save forstudent\n",
      "To run this system prompt again:\n",
      "ollama run forstudent\n",
      "Integration with LangChain\n",
      "Ollama can be used with LangChain, a tool that enables complex interactions with LLMs. To get started with LangChain and Ollama, first, pull the required model:\n",
      "ollama pull llama3\n",
      "Then, install the necessary packages:\n",
      "pip install langchain langchain-ollama ollama\n",
      "You can interact with the model through code, such as invoking a basic conversation:\n",
      "from langchain_ollama import OllamaLLM\n",
      "model = OllamaLLM(model=\"llama3\")\n",
      "response = model.invoke(input=\"What's up?\")\n",
      "print(response)\n",
      "The model might respond with something like:\n",
      "\"Not much! Just an AI, waiting to chat with you. How about you? What's new and exciting in your world?\"\n",
      "Building a Simple Chatbot\n",
      "Using LangChain, you can also build a simple AI chatbot:\n",
      "```\n",
      "from langchain_ollama import OllamaLLM\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "template = \"\"\"\n",
      "User will ask you questions. Answer it.\n",
      "The history of this conversation: {context}\n",
      "Question: {question}\n",
      "Answer: \n",
      "\"\"\"\n",
      "model = OllamaLLM(model=\"llama3\")\n",
      "prompt = ChatPromptTemplate.from_template(template)\n",
      "chain = prompt | model\n",
      "def chat():\n",
      "    context = \"\"\n",
      "    print(\"Welcome to the AI Chatbot! Type 'exit' to quit.\")\n",
      "    while True:\n",
      "        question = input(\"You: \")\n",
      "        if question.lower() == \"exit\":\n",
      "            break\n",
      "        response = chain.invoke({\"context\":context, \"question\": question})\n",
      "        print(f\"AI: {response}\")\n",
      "        context += f\"\\nUser: {question}\\nAI: {response}\"\n",
      "chat()\n",
      "```\n",
      "This will create an interactive chatbot session where you can ask the AI questions, and it will respond accordingly. For example:\n",
      "You: What's up?\n",
      "AI: Not much, just getting started on my day. How about you?\n",
      "Using AnythingLLM with Ollama\n",
      "AnythingLLM is another useful tool that acts as an AI agent and RAG (retrieval-augmented generation) tool, which can also run locally. To try this out, pull a model, such as:\n",
      "ollama pull llama3:8b-instruct-q8_0\n",
      "In AnythingLLM, you can select Ollama in the preferences and assign a name to your workspace. Although running models can be slow, the system works efficiently once set up.\n",
      "You can also interact with Ollama via a web UI by following the installation instructions provided.\n",
      "For more details, visit Ollama’s official pages and documentation to explore the full range of features and models available.\n",
      "Best tools available, but unheard\n",
      "Several alternatives and complementary tools to LangChain and AnythingLLM provide capabilities for working with language models (LLMs) and building AI-powered applications. These tools help orchestrate interactions with LLMs, enabling more advanced AI-driven workflows, automating tasks, or integrating AI into various applications. Here are some notable examples:\n",
      "1. Haystack by Deepset\n",
      "Haystack is an open-source framework that builds search engines and question-answering systems using LLMs. It enables developers to connect different components, such as retrievers, readers, and generators, to create an information retrieval pipeline.\n",
      "Key Features:\n",
      "\n",
      "\n",
      "Offers a pipeline-based approach for search, Q&A, and generative tasks.\n",
      "\n",
      "\n",
      "Supports integration with models from Hugging Face, OpenAI, and local models. Can combine LLMs with external data sources such as databases, knowledge graphs, and APIs.\n",
      "\n",
      "\n",
      "Great for production-grade applications with robust scalability and reliability.\n",
      "\n",
      "\n",
      "Link: Haystack GitHub\n",
      "2. LlamaIndex (formerly GPT Index)\n",
      "LlamaIndex (formerly GPT Index) is a data framework that helps you index and retrieve information efficiently from large datasets using LLMs. It's designed to handle document-based workflows by structuring data, indexing it, and enabling retrieval when interacting with LLMs.\n",
      "Key Features:\n",
      "\n",
      "\n",
      "Integrates with external data sources such as PDFs, HTML, CSVs, or custom APIs.\n",
      "\n",
      "\n",
      "Builds on top of LLMs for more efficient data querying and document summarization. Helps optimize the performance of LLMs by constructing memory-efficient indices.\n",
      "\n",
      "\n",
      "Provides compatibility with LangChain and other frameworks.\n",
      "\n",
      "\n",
      "Link: LlamaIndex GitHub\n",
      "3. Chroma\n",
      "Chroma is an open-source embedding database designed for LLMs. It helps store and query high-dimensional vector embeddings of data, enabling you to work with semantic search, retrieval-augmented generation (RAG), and more.\n",
      "Key Features:\n",
      "\n",
      "\n",
      "Embedding search for documents or large datasets using models like OpenAI or Hugging Face transformers.\n",
      "\n",
      "\n",
      "Scalable and optimized for efficient retrieval of large datasets with millisecond latency.\n",
      "\n",
      "\n",
      "Works well for semantic search, content recommendations, or building conversational agents.\n",
      "\n",
      "\n",
      "Link: Chroma GitHub\n",
      "4. Hugging Face Transformers\n",
      "Hugging Face provides a library of pretrained transformers that can be used for various NLP tasks such as text generation, question-answering, and classification. It offers easy integration with LLMs, making it a great tool for working with different models in a unified way.\n",
      "Key Features:\n",
      "\n",
      "\n",
      "Supports a wide range of models, including GPT, BERT, T5, and custom models.\n",
      "\n",
      "\n",
      "Provides pipelines for quick setup of tasks like Q&A, summarization, and translation.\n",
      "\n",
      "\n",
      "Hugging Face Hub hosts a large variety of pre-trained models ready for deployment.\n",
      "\n",
      "\n",
      "Link: Hugging Face Transformers\n",
      "5. Pinecone\n",
      "Pinecone is a managed vector database that allows you to store, index, and query large-scale vectors produced by LLMs. It is designed for high-speed semantic search, vector search, and machine-learning applications.\n",
      "Key Features:\n",
      "\n",
      "\n",
      "Fast, scalable, and reliable vector search for applications requiring high performance.\n",
      "\n",
      "\n",
      "Integrates seamlessly with LLMs to power retrieval-based models.\n",
      "\n",
      "\n",
      "Handles large datasets and enables search across millions or billions of vectors.\n",
      "\n",
      "\n",
      "Link: Pinecone Website\n",
      "6. OpenAI API\n",
      "OpenAI’s API gives access to a wide range of LLMs, including the GPT series (like GPT-3.5 and GPT-4). It provides text generation, summarization, translation, and code generation capabilities.\n",
      "Key Features:\n",
      "\n",
      "\n",
      "Access to state-of-the-art models like GPT-4 and DALL-E for image generation.\n",
      "\n",
      "\n",
      "Offers prompt engineering for fine-tuning and controlling model behavior.\n",
      "\n",
      "\n",
      "Simplifies AI integration into applications without needing to manage infrastructure.\n",
      "\n",
      "\n",
      "Link: OpenAI API\n",
      "7. Rasa\n",
      "Rasa is an open-source framework for building conversational AI assistants and chatbots. It allows for highly customizable AI assistants trained on specific tasks and workflows, making it a good alternative to pre-trained LLM chatbots.\n",
      "Key Features:\n",
      "\n",
      "\n",
      "Supports NLU (Natural Language Understanding) and dialogue management.\n",
      "\n",
      "\n",
      "Highly customizable for domain-specific applications.\n",
      "\n",
      "\n",
      "Can integrate with LLMs to enhance chatbot capabilities.\n",
      "\n",
      "\n",
      "Link: Rasa Website\n",
      "8. Cohere\n",
      "Cohere offers NLP APIs and large-scale language models similar to OpenAI. It focuses on tasks like classification, text generation, and search, providing a powerful platform for LLM-based applications.\n",
      "Key Features:\n",
      "\n",
      "\n",
      "Provides easy access to LLMs through an API, allowing developers to implement NLP tasks quickly.\n",
      "\n",
      "\n",
      "Offers fine-tuning options for domain-specific applications.\n",
      "\n",
      "\n",
      "Well-suited for tasks like customer support automation and text classification.\n",
      "\n",
      "\n",
      "Link: Cohere Website\n",
      "9. Vercel AI SDK\n",
      "Vercel AI SDK provides tools for building AI-powered applications using frameworks like Next.js. It simplifies the development process by integrating APIs from OpenAI, Hugging Face, and other AI providers into web applications.\n",
      "Key Features:\n",
      "\n",
      "\n",
      "Seamless integration with AI models in serverless environments.\n",
      "\n",
      "\n",
      "Supports building interactive applications with fast deployments using Vercel’s infrastructure.\n",
      "\n",
      "\n",
      "Focuses on web-based applications and LLM-powered front-end experiences.\n",
      "\n",
      "\n",
      "Link: Vercel AI SDK\n",
      "\n",
      "Conclusion\n",
      "Beyond LangChain and AnythingLLM, many powerful tools and frameworks cater to different needs when working with LLMs. Whether you want to build conversational agents, semantic search engines, or specialized AI applications, platforms like Haystack, LlamaIndex, Chroma, and others offer flexible and scalable solutions. Depending on your specific use case, you can choose the most suitable tool for integrating LLMs into your projects.\n",
      "#languagemodellingWeb3BlockchaindecentralizationAIArtificial IntelligenceollamallmSpheron\n",
      "Share this\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tool_choice = json.loads(res[\"message\"][\"content\"])[\"action\"][\"tool\"]\n",
    "args = json.loads(res[\"message\"][\"content\"])[\"action\"][\"args\"]\n",
    "\n",
    "# we use a dictionary to map the tool name to the tool function\n",
    "tool_selector = {x.__name__: x for x in tools}\n",
    "\n",
    "# now we select the tool and call it with the arguments\n",
    "observation = tool_selector[tool_choice](**args)\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Within the ReAct framework we would then pass this information back to our LLM via a new `observation` variable. Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"thought\": \"I need to find out what Ollama refers to in the context of AI\",\n",
      "  \"action\": {\n",
      "    \"tool\": \"search\",\n",
      "    \"args\": {\n",
      "      \"query\": \"Ollama AI\"\n",
      "    }\n",
      "  },\n",
      "  \"observation\": \"Run Language Models Locally with Ollama: A Comprehensive Guide\\n\\n\\nFollow\\n\\nFollow\\n\\n\\nRun Language Models Locally with Ollama: A Comprehensive Guide\\n\\nSpheron Network\\n\\u00b7Nov 3, 2024\\u00b77 min read\\nTable of contents\\n\\nIntegration with LangChain\\nBuilding a Simple Chatbot\\nUsing AnythingLLM with Ollama\\nBest tools available, but unheard\\n1. Haystack by Deepset\\n2. LlamaIndex (formerly GPT Index)\\n3. Chroma\\n4. Hugging Face Transformers\\n5. Pinecone\\n6. OpenAI API\\n7. Rasa\\n8. Cohere\\n9. Vercel AI SDK\\n\\n\\nConclusion\\n\\nOllama is an open-source platform that simplifies the process of setting up and running large language models (LLMs) on your local machine. With Ollama, you can easily download, install, and interact with LLMs without the usual complexities.\\nTo get started, you can download Ollama from here. Once installed, open a terminal and type:\\nollama run phi3\\nOR\\nollama pull phi3\\nollama run phi3\\nThis will download the required layers of the model \\\"phi3\\\". After the model is loaded, Ollama enters a REPL (Read-Eval-Print Loop), which is an interactive environment where you can input commands and see immediate results.\\nTo explore the available commands within the REPL, type:\\n/?\\nThis will show you a list of commands you can use. For example, to exit the REPL, type /bye. You can also display the models you have installed using:\\nollama ls\\nIf you need to remove any model, use:\\nollama rm\\nFor a complete list of available models in Ollama, you can visit their model library, which contains details about model sizes, parameters, and more. Additionally, Ollama has specific hardware requirements. For instance, to run a 7B model, you'll need at least 8 GB of RAM; 16 GB for a 13B model, and 32 GB for a 33B model. If you have a GPU, Ollama supports it\\u2014more details can be found on their GitHub page. However, if you're running on a CPU, expect it to perform slower.\\nOllama also allows you to set a custom system prompt. For example, to instruct the system to explain concepts at a basic level, you can use:\\n/set system Explain concepts as if you are talking to a primary school student.\\nYou can then save and reuse this setup by giving it a name:\\n/save forstudent\\nTo run this system prompt again:\\nollama run forstudent\\nIntegration with LangChain\\nOllama can be used with LangChain, a tool that enables complex interactions with LLMs. To get started with LangChain and Ollama, first, pull the required model:\\nollama pull llama3\\nThen, install the necessary packages:\\npip install langchain langchain-ollama ollama\\nYou can interact with the model through code, such as invoking a basic conversation:\\nfrom langchain_ollama import OllamaLLM\\nmodel = OllamaLLM(model=\\\"llama3\\\")\\nresponse = model.invoke(input=\\\"What's up?\\\")\\nprint(response)\\nThe model might respond with something like:\\n\\\"Not much! Just an AI, waiting to chat with you. How about you? What's new and exciting in your world?\\\"\\nBuilding a Simple Chatbot\\nUsing LangChain, you can also build a simple AI chatbot:\\n```\\nfrom langchain_ollama import OllamaLLM\\nfrom langchai...\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "iteration = json.loads(step)\n",
    "# we limit the observation to 3000 characters to avoid overwhelming the LLM\n",
    "iteration[\"observation\"] = observation[:3000] + \"...\"\n",
    "iteration_str = json.dumps(iteration, indent=2)\n",
    "print(iteration_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed this back into our chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"thought\": \"I need to provide an answer based on what I found\", \"action\": {\"tool\": \"answer\", \"args\": {\"answer\": \"Ollama is an open-source platform that simplifies the process of setting up and running large language models (LLMs) on your local machine.\"}} }\n"
     ]
    }
   ],
   "source": [
    "res = ollama.chat(\n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"{system_prompt}\\n\\n{tool_instructions}\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is Ollama in the context of AI?\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"Step 1:\\n{iteration_str}\\n\\nWhat do I do next to answer the user's question...\"},\n",
    "    ],\n",
    "    format=\"json\",\n",
    "    options={\"temperature\": 0.0}\n",
    ")\n",
    "\n",
    "step2 = res[\"message\"][\"content\"]\n",
    "print(step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! Our final answer from the LLM is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is an open-source platform that simplifies the process of setting up and running large language models (LLMs) on your local machine.\n"
     ]
    }
   ],
   "source": [
    "step2_json = json.loads(step2)\n",
    "print(step2_json[\"action\"][\"args\"][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, now let's take everything we've done so far and use it to construct our ReAct agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "class ReActAgent:\n",
    "    def __init__(self, tools: list[Callable]):\n",
    "        self.messages = []\n",
    "        self.tools = {x.__name__: x for x in tools}\n",
    "        tool_instructions = self._format_tool_instructions(tools=tools)\n",
    "        self.system_prompt = system_prompt\n",
    "        # add system prompt and tool instructions to our messages\n",
    "        self.messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"{self.system_prompt}\\n\\n{tool_instructions}\"\n",
    "        })\n",
    "\n",
    "    def _format_tool_instructions(self, tools: list[Callable]):\n",
    "        # get the various parameters/description from each tool function\n",
    "        tool_descriptions = [\n",
    "            {\n",
    "                \"name\": tool.__name__,\n",
    "                \"description\": str(inspect.getdoc(tool)),\n",
    "                \"args\": {\n",
    "                    k: str(v).split(\": \")[1] for k, v in inspect.signature(tool).parameters.items()\n",
    "                }\n",
    "            } for tool in tools\n",
    "        ]\n",
    "        # parse these into text instructions that can be added to our system prompt\n",
    "        tool_instructions = (\n",
    "            \"You have access to the following tools ONLY, no other tools exist:\\n\\n\"\n",
    "            + \"\\n\".join([str(x) for x in tool_descriptions])\n",
    "        )\n",
    "        return tool_instructions\n",
    "\n",
    "    def __call__(self, prompt: str, max_steps: int = 3):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        step_count = 1\n",
    "        steps = []\n",
    "        while step_count < max_steps:\n",
    "            # get the next step\n",
    "            step_dict = self._call_llm(\n",
    "                messages=self.messages+self._format_scratchpad(steps)\n",
    "            )\n",
    "            # get the tool choice and arguments\n",
    "            tool_choice = step_dict[\"action\"][\"tool\"]\n",
    "            args = step_dict[\"action\"][\"args\"]\n",
    "            self._print_react(step_count=step_count, step_dict=step_dict)\n",
    "            if tool_choice == \"answer\":\n",
    "                # we've reached the final step\n",
    "                self.messages.append({\"role\": \"assistant\", \"content\": json.dumps(step_dict)})\n",
    "                return step_dict[\"action\"][\"args\"][\"answer\"]\n",
    "            else:\n",
    "                # otherwise we call the chosen tool\n",
    "                observation = self.tools[tool_choice](**args).strip()\n",
    "                print(f\"Observation {step_count}: {observation[:200]}... ({len(observation)} chars)\")\n",
    "                # if the observation is very long we truncate it\n",
    "                if len(observation) > 3000:\n",
    "                    observation = observation[:3000] + \"...\"\n",
    "            # add the step to our scratchpad\n",
    "            steps.append({\n",
    "                \"thought\": step_dict[\"thought\"],\n",
    "                \"action\": step_dict[\"action\"],\n",
    "                \"observation\": observation\n",
    "            })\n",
    "            step_count += 1\n",
    "        # if we get here we've hit the max steps so we force the answer tool\n",
    "        # to do this we modify the system prompt to only show the answer tool\n",
    "        print(f\"Exceeded max_steps={max_steps}, forcing early answer.\")\n",
    "        messages = self.messages.copy()\n",
    "        tool_instructions = self._format_tool_instructions(tools=[self.tools[\"answer\"]])\n",
    "        messages[0][\"content\"] = f\"{self.system_prompt}\\n\\n{tool_instructions}\"\n",
    "        # now we call the LLM with the modified system prompt\n",
    "        step_dict = self._call_llm(messages=messages)\n",
    "        return step_dict[\"action\"][\"args\"][\"answer\"]\n",
    "\n",
    "    def _call_llm(self, messages: list[dict]) -> dict:\n",
    "        res = ollama.chat(\n",
    "            model=\"llama3.2:3b-instruct-fp16\",\n",
    "            messages=messages,\n",
    "            format=\"json\",\n",
    "            options={\"temperature\": 0.0},\n",
    "        )\n",
    "        step_dict = json.loads(res[\"message\"][\"content\"])\n",
    "        return step_dict\n",
    "        \n",
    "    def _format_scratchpad(self, steps: list[dict]) -> list[dict]:\n",
    "        if not steps:\n",
    "            # no steps so we just return an empty list\n",
    "            return []\n",
    "        steps_str = \"\"\n",
    "        for i, step in enumerate(steps):\n",
    "            steps_str += f\"Step {i+1}:\\n{json.dumps(step, indent=2)}\\n\\n\"\n",
    "        steps_str += \"What do I do next to answer the user's question...\"\n",
    "        return [{\"role\": \"assistant\", \"content\": steps_str}]\n",
    "\n",
    "    def _print_react(self, step_count: int, step_dict: dict) -> None:\n",
    "        \"\"\"Prints the Reasoning (thought) and Action step\"\"\"\n",
    "        react = \"\\n\".join([\n",
    "            f\"Thought {step_count}: {step_dict['thought']}\",\n",
    "            f\"Action {step_count}: {step_dict['action']}\",\n",
    "        ])\n",
    "        print(react)\n",
    "\n",
    "# initialize our agent with the search and answer tools\n",
    "agent = ReActAgent(tools=[search, answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1: I need to find out what Ollama refers to in the context of AI\n",
      "Action 1: {'tool': 'search', 'args': {'query': 'Ollama AI'}}\n",
      "Observation 1: Run Language Models Locally with Ollama: A Comprehensive Guide\n",
      "\n",
      "\n",
      "Follow\n",
      "\n",
      "Follow\n",
      "\n",
      "\n",
      "Run Language Models Locally with Ollama: A Comprehensive Guide\n",
      "\n",
      "Spheron Network\n",
      "·Nov 3, 2024·7 min read\n",
      "Table of conte... (10390 chars)\n",
      "Thought 2: I need to provide an answer based on what I found\n",
      "Action 2: {'tool': 'answer', 'args': {'answer': 'Ollama is an open-source platform that simplifies the process of setting up and running large language models (LLMs) on your local machine.'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ollama is an open-source platform that simplifies the process of setting up and running large language models (LLMs) on your local machine.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"What is Ollama in the context of AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin a new conversation, we must reinitialize our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1: Chain-of-thought prompting refers to a technique used in AI where the model generates a series of intermediate steps or thoughts that lead to the final answer. This approach allows the model to provide more transparency and explainability into its decision-making process.\n",
      "Action 1: {'tool': 'search', 'args': {'query': 'Chain-of-thought prompting in AI'}}\n",
      "Observation 1: What Is Chain-of-Thought Prompting and How Can You Use It?\n",
      "Maxwell Timothy\n",
      "10 min read\n",
      "Chain-of-Thought Prompting.\n",
      "What is it? How does it matter in AI, especially within the expanding field of prompt... (12562 chars)\n",
      "Thought 2: I need to provide a clear explanation of Chain-of-Thought prompting and its benefits\n",
      "Action 2: {'tool': 'answer', 'args': {'answer': 'Chain-of-thought prompting is a technique used in AI where the model generates a series of intermediate steps or thoughts that lead to the final answer, allowing for more transparency and explainability into its decision-making process.'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chain-of-thought prompting is a technique used in AI where the model generates a series of intermediate steps or thoughts that lead to the final answer, allowing for more transparency and explainability into its decision-making process.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = ReActAgent(tools=[search, answer])\n",
    "\n",
    "agent(\"What is Chain-of-Thought prompting in AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try another query, this time `\"What are AI agents?\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1: I need to define what an AI agent is\n",
      "Action 1: {'tool': 'search', 'args': {'query': 'definition of AI agent'}}\n",
      "Observation 1: What Are AI Agents?\n",
      "AI agents are poised to revolutionize the way we live and work, automating tasks normally completed by humans.\n",
      "AI agents are artificial intelligence systems that can perform a wide... (12564 chars)\n",
      "Thought 2: I need to summarize what an AI agent is\n",
      "Action 2: {'tool': 'answer', 'args': {'answer': 'An AI agent is a type of artificial intelligence system that can perform complex tasks independently, without the need for fixed rules or constant human intervention.'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'An AI agent is a type of artificial intelligence system that can perform complex tasks independently, without the need for fixed rules or constant human intervention.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = ReActAgent(tools=[search, answer])\n",
    "\n",
    "agent(\"What are AI agents?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask follow up questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1: I need to identify a well-known type of AI agent\n",
      "Action 1: {'tool': 'search', 'args': {'query': 'popular types of AI agents'}}\n",
      "Observation 1: Sign up\n",
      "Sign in\n",
      "Sign up\n",
      "Sign in\n",
      "Guide of AI Agent Types with examples\n",
      "Thomas Latterner\n",
      "Follow\n",
      "--\n",
      "Listen\n",
      "Share\n",
      "From a home alarm, to a fleet of robots in a warehouse, to your smartphone’s assistant, AI... (6343 chars)\n",
      "Thought 2: I need to provide more information about a popular type of AI agent\n",
      "Action 2: {'tool': 'answer', 'args': {'answer': 'A popular type of AI agent is a Learning Agent, which enhances its performance over time through experience and learning from data.'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A popular type of AI agent is a Learning Agent, which enhances its performance over time through experience and learning from data.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"and what is a popular type?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite us not specifying that we're asking about AI agent types in our query (we just ask for `\"popular type?\"`) the LLM understands that this is a follow-up question from our previous message and so it reformulates our query to include that important context. In this case, rewriting our vague question to a more explicit `\"popular types of AI agents\"`.\n",
    "\n",
    "_It's worth noting that the agent types mentioned in the retrieved article originate from agents in the context of **R**einforcement **L**earning (RL). So terminology such as **Learning Agents** are not typical when discussing LLM-based agents \\[[source](https://www.javatpoint.com/types-of-ai-agents)\\]._\n",
    "\n",
    "Let's try one more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1: RAG stands for Relevance, Accuracy, and Generalizability. It is a framework used in natural language processing (NLP) to evaluate the quality of text-based models.\n",
      "Action 1: {'tool': 'answer', 'args': {'answer': 'RAG is a framework used in NLP to evaluate the quality of text-based models.'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RAG is a framework used in NLP to evaluate the quality of text-based models.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = ReActAgent(tools=[search, answer])\n",
    "\n",
    "agent(\"Give me a deep dive on RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the agent hallucinated. It begun by *incorrectly* stating that RAG stands for **R**elevance, **A**ccuracy, and **G**eneralizability. Because of the LLM's overconfidence in this initial answer it does not defer to the `search` tool, where it would likely find that RAG more commonly means **R**etrieval **A**ugmented **G**eneration.\n",
    "\n",
    "There are many ways we can make our agents more resilient to producing bad outputs. First and foremost is to choose a more capable LLM. In our example we are using a 3B parameter LLM which is (in the world of LLMs) a *tiny* model. Tiny models can run on smaller hardware and are faster, but are less capable and prone to hallucination, not following instructions, or losing track of the original objective over multiple reasoning steps.\n",
    "\n",
    "There are always other options too, we could try to improve our prompting, add more determinstic checks for erronous outputs and run retries, or we can try to improve the quality of data being fed into our LLM via our tools. All of these are options open to us while developing with LLMs and we'll discussed these and other options during future chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
