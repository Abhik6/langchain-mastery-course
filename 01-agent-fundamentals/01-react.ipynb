{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be modifying ReAct a little to take advantage of the progress that has been made in LLMs since the paper was originally released. We make two modifications:\n",
    "\n",
    "1. We use chat models, when ReAct was released LLMs were not fine-tuned specifically for chat and instead were prompted to generate chat-like dialogues. Most SotA LLMs nowadays are built specifically for chat and so the input into them must be modified to be chat-model friendly.\n",
    "\n",
    "2. We will use JSON-mode to force structured output from our LLMs. The original ReAct method simply instructed the LLM to output everything in a particular format. That works but is prone to occasionally breaking. By forcing JSON-like output we reduce the likelihood of poorly structured output. To accomodate this we modify the instructions to ask for `thought` and `action` steps in a JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant. Given a user query you must provide a `thought` and\n",
    "`action` step that take one step towards solving the user's query. Both the\n",
    "`thought` and `action` steps will be contained in JSON output.\n",
    "\n",
    "The `thought` is the first key mapping to your reasoning on how to solve the\n",
    "user's query.\n",
    "\n",
    "The `action` step is the second key mapping that describes how you wish to use\n",
    "the chosen tool to solve the user's query. It contains a `tool` key mapping to\n",
    "the name of the tool to use and a `args` key containing a JSON object of\n",
    "arguments to pass to the tool.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "user: What is the weather in Tokyo?\n",
    "assistant: {\n",
    "  \"thought\": \"I need to find out the current temperature in Tokyo\",\n",
    "  \"action\": {\"tool\": \"search\", \"args\": {\"query\": \"current temperature in Tokyo\"}}\n",
    "}\n",
    "\n",
    "If you have performed any previous thought and action steps, you will find them\n",
    "below under the \"Previous Steps\" section. Alongside these you will find an\n",
    "`observation` key containing the output of those previous actions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't defined any tools or agent logic yet, but let's see what type of output\n",
    "our LLM produces if we prompt it with this system prompt.\n",
    "\n",
    "Make sure you have Llama 3.2 downloaded already via:\n",
    "\n",
    "```\n",
    "ollama run llama3.2:3b-instruct-fp16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"thought\": \"I need to determine the current date\",\n",
      "  \"action\": {\n",
      "    \"tool\": \"calendar.today()\",\n",
      "    \"args\": {}\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "res = ollama.chat(\n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"What is the date today?\"},\n",
    "    ],\n",
    "    format=\"json\",\n",
    ")\n",
    "\n",
    "print(res[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we're outputting the correct format *however* we don't have a `date` tool, in fact, we don't have *any* tools! Let's define some.\n",
    "\n",
    "First, we'll define a search tool. This tool will allow our agent to search the web for information. To implement it we will use the Tavily API, it comes with a number of requests for free but we do need to [sign up for the API](https://app.tavily.com/home) and get an API key to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the weather in Tokyo?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'title': 'Weather in Tokyo',\n",
       "   'url': 'https://www.weatherapi.com/',\n",
       "   'content': \"{'location': {'name': 'Tokyo', 'region': 'Tokyo', 'country': 'Japan', 'lat': 35.6895, 'lon': 139.6917, 'tz_id': 'Asia/Tokyo', 'localtime_epoch': 1730666031, 'localtime': '2024-11-04 05:33'}, 'current': {'last_updated_epoch': 1730665800, 'last_updated': '2024-11-04 05:30', 'temp_c': 15.2, 'temp_f': 59.4, 'is_day': 0, 'condition': {'text': 'Clear', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 2.2, 'wind_kph': 3.6, 'wind_degree': 146, 'wind_dir': 'SSE', 'pressure_mb': 1023.0, 'pressure_in': 30.21, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 72, 'cloud': 0, 'feelslike_c': 15.2, 'feelslike_f': 59.4, 'windchill_c': 16.6, 'windchill_f': 61.9, 'heatindex_c': 16.6, 'heatindex_f': 61.9, 'dewpoint_c': 9.1, 'dewpoint_f': 48.4, 'vis_km': 10.0, 'vis_miles': 6.0, 'uv': 0.0, 'gust_mph': 3.2, 'gust_kph': 5.2}}\",\n",
       "   'score': 0.9989411,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Weather in Tokyo in March 2024 (Tōkyō-to) - Detailed Weather Forecast ...',\n",
       "   'url': 'https://world-weather.info/forecast/japan/tokyo/march-2024/',\n",
       "   'content': 'Detailed ⚡ Tokyo Weather Forecast for March 2024 - day/night 🌡️ temperatures, precipitations - World-Weather.info ... 11 +54° +41° 12 +50° +50° 13 ... Extended weather forecast in Tokyo. Hourly Week 10-Day 14-Day 30-Day Year.',\n",
       "   'score': 0.998862,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Tokyo, Tokyo, Japan Weather Forecast | AccuWeather',\n",
       "   'url': 'https://www.accuweather.com/en/jp/tokyo/226396/weather-forecast/226396',\n",
       "   'content': 'Get the latest weather forecast for Tokyo, Japan, with hourly, daily, and 15-day details. Check the temperature, humidity, wind, rain, and snow from AccuWeather.',\n",
       "   'score': 0.9380106,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Tokyo, Tokyo, Japan Daily Weather | AccuWeather',\n",
       "   'url': 'https://www.accuweather.com/en/jp/tokyo/226396/daily-weather-forecast/226396',\n",
       "   'content': \"Know what's coming with AccuWeather's extended daily forecasts for Tokyo, Tokyo, Japan. Up to 90 days of daily highs, lows, and precipitation chances.\",\n",
       "   'score': 0.86476797,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Tokyo, Tokyo, Japan Monthly Weather | AccuWeather',\n",
       "   'url': 'https://www.accuweather.com/en/jp/tokyo/226396/october-weather/226396',\n",
       "   'content': 'Get the monthly weather forecast for Tokyo, Tokyo, Japan, including daily high/low, historical averages, to help you plan ahead.',\n",
       "   'score': 0.83400166,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 3.19}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "TAVILY_API_KEY = \"tvly-...\"\n",
    "\n",
    "tavily_url = \"https://api.tavily.com\"\n",
    "\n",
    "res = requests.post(\n",
    "    f\"{tavily_url}/search\",\n",
    "    json={\n",
    "        \"api_key\": TAVILY_API_KEY,\n",
    "        \"query\": \"What is the weather in Tokyo?\"\n",
    "    },\n",
    ")\n",
    "\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we don't return much useful info here beyond that we have some URLs that *do contain* the information we need. Fortunately, we can extract this information via Tavily's `/extract` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [x[\"url\"] for x in res.json()[\"results\"]]\n",
    "\n",
    "res = requests.post(\n",
    "    f\"{tavily_url}/extract\",\n",
    "    json={\n",
    "        \"api_key\": TAVILY_API_KEY,\n",
    "        \"urls\": urls\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': [{'url': 'https://world-weather.info/forecast/japan/tokyo/march-2024/',\n",
       "   'raw_content': \"Weather in Tokyo in March 2024 (Tōkyō-to) - Detailed Weather Forecast for a Month\\n\\nAdd the current city\\nSearch\\n\\nWeather\\nArchive\\nWidgets\\n\\n°F\\n\\nWorld\\nJapan\\nTōkyō-to\\nWeather in Tokyo\\n\\nWeather in Tokyo in March 2024\\nTokyo Weather Forecast for March 2024 is based on statistical data.\\n20152016201720182019202020212022202320242025\\nJanFebMarAprMayJunJulAugSepOctNovDec\\nMarch\\nStart Week On\\nSunday\\nMonday\\n\\nSun\\nMon\\nTue\\nWed\\nThu\\nFri\\n\\nSat\\n\\n\\n1 +57°+43°\\n\\n2 +48°+45°\\n3 +54°+39°\\n4 +55°+45°\\n5 +46°+45°\\n6 +48°+37°\\n7 +50°+43°\\n8 +46°+39°\\n9 +52°+41°\\n10 +52°+41°\\n11 +54°+41°\\n12 +50°+50°\\n13 +54°+48°\\n14 +57°+43°\\n15 +59°+46°\\n16 +63°+54°\\n17 +66°+54°\\n18 +54°+52°\\n19 +54°+45°\\n20 +52°+48°\\n21 +50°+43°\\n22 +54°+41°\\n23 +50°+45°\\n24 +57°+45°\\n25 +54°+52°\\n26 +52°+52°\\n27 +57°+46°\\n28 +57°+48°\\n29 +66°+57°\\n30 +68°+59°\\n31 +73°+59°\\n\\nAverage weather in March 2024\\n7 days\\nPrecipitation\\n10 days\\nCloudy\\n14 days\\nSunny\\nDay\\n+55\\n°F\\nNight\\n+47\\n°F\\nCompare with another month\\nExtended weather forecast in Tokyo\\nHourlyWeek10-Day14-Day30-DayYear\\n\\nWeather in large and nearby cities\\nKawaguchi+59°\\nFuchū+50°\\nKashiwa+48°\\nTokorozawa+50°\\nKasukabe+59°\\nAgeo+59°\\nKawagoe+52°\\nYokohama+59°\\nMachida+50°\\nKawasaki+59°\\nKodaira+50°\\nFunabashi+59°\\nNerima+59°\\nSōka+59°\\nIchikawa+59°\\nMatsudo+59°\\nKoshigaya+59°\\nKami-renjaku+59°\\nMinimum and maximum\\nworld's temperature today\\nRussia\\nVerkhoyansk\\nday\\n-27°F\\nnight\\n-36°F\\nPakistan\\nTurbat\\nday\\n+108°F\\nnight\\n+68°F\\nWeather forecast on your site Install Tokyo+54°\\nServices\\nSupport\\n\\nUser agreement\\nFeedback\\nAbout Us\\n\\nSearch\\nCity or place…\\nCopyright © 2024 «World-Weather.info» All rights reserved. Privacy policy\\nTemperature units\\n°F°C\\nIcons\\n\"}],\n",
       " 'failed_results': [{'url': 'https://www.weatherapi.com/',\n",
       "   'error': 'Failed to get content'},\n",
       "  {'url': 'https://www.accuweather.com/en/jp/tokyo/226396/weather-forecast/226396',\n",
       "   'error': 'Request timed out'},\n",
       "  {'url': 'https://www.accuweather.com/en/jp/tokyo/226396/daily-weather-forecast/226396',\n",
       "   'error': 'Request timed out'},\n",
       "  {'url': 'https://www.accuweather.com/en/jp/tokyo/226396/october-weather/226396',\n",
       "   'error': 'Request timed out'}],\n",
       " 'response_time': 6.82}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most requests didn't work, but that's okay we made multiple requests and fortunately received one result that looks perfect, we can extract that information like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather in Tokyo in March 2024 (Tōkyō-to) - Detailed Weather Forecast for a Month\n",
      "\n",
      "Add the current city\n",
      "Search\n",
      "\n",
      "Weather\n",
      "Archive\n",
      "Widgets\n",
      "\n",
      "°F\n",
      "\n",
      "World\n",
      "Japan\n",
      "Tōkyō-to\n",
      "Weather in Tokyo\n",
      "\n",
      "Weather in Tokyo in March 2024\n",
      "Tokyo Weather Forecast for March 2024 is based on statistical data.\n",
      "20152016201720182019202020212022202320242025\n",
      "JanFebMarAprMayJunJulAugSepOctNovDec\n",
      "March\n",
      "Start Week On\n",
      "Sunday\n",
      "Monday\n",
      "\n",
      "Sun\n",
      "Mon\n",
      "Tue\n",
      "Wed\n",
      "Thu\n",
      "Fri\n",
      "\n",
      "Sat\n",
      "\n",
      "\n",
      "1 +57°+43°\n",
      "\n",
      "2 +48°+45°\n",
      "3 +54°+39°\n",
      "4 +55°+45°\n",
      "5 +46°+45°\n",
      "6 +48°+37°\n",
      "7 +50°+43°\n",
      "8 +46°+39°\n",
      "9 +52°+41°\n",
      "10 +52°+41°\n",
      "11 +54°+41°\n",
      "12 +50°+50°\n",
      "13 +54°+48°\n",
      "14 +57°+43°\n",
      "15 +59°+46°\n",
      "16 +63°+54°\n",
      "17 +66°+54°\n",
      "18 +54°+52°\n",
      "19 +54°+45°\n",
      "20 +52°+48°\n",
      "21 +50°+43°\n",
      "22 +54°+41°\n",
      "23 +50°+45°\n",
      "24 +57°+45°\n",
      "25 +54°+52°\n",
      "26 +52°+52°\n",
      "27 +57°+46°\n",
      "28 +57°+48°\n",
      "29 +66°+57°\n",
      "30 +68°+59°\n",
      "31 +73°+59°\n",
      "\n",
      "Average weather in March 2024\n",
      "7 days\n",
      "Precipitation\n",
      "10 days\n",
      "Cloudy\n",
      "14 days\n",
      "Sunny\n",
      "Day\n",
      "+55\n",
      "°F\n",
      "Night\n",
      "+47\n",
      "°F\n",
      "Compare with another month\n",
      "Extended weather forecast in Tokyo\n",
      "HourlyWeek10-Day14-Day30-DayYear\n",
      "\n",
      "Weather in large and nearby cities\n",
      "Kawaguchi+59°\n",
      "Fuchū+50°\n",
      "Kashiwa+48°\n",
      "Tokorozawa+50°\n",
      "Kasukabe+59°\n",
      "Ageo+59°\n",
      "Kawagoe+52°\n",
      "Yokohama+59°\n",
      "Machida+50°\n",
      "Kawasaki+59°\n",
      "Kodaira+50°\n",
      "Funabashi+59°\n",
      "Nerima+59°\n",
      "Sōka+59°\n",
      "Ichikawa+59°\n",
      "Matsudo+59°\n",
      "Koshigaya+59°\n",
      "Kami-renjaku+59°\n",
      "Minimum and maximum\n",
      "world's temperature today\n",
      "Russia\n",
      "Verkhoyansk\n",
      "day\n",
      "-27°F\n",
      "night\n",
      "-36°F\n",
      "Pakistan\n",
      "Turbat\n",
      "day\n",
      "+108°F\n",
      "night\n",
      "+68°F\n",
      "Weather forecast on your site Install Tokyo+54°\n",
      "Services\n",
      "Support\n",
      "\n",
      "User agreement\n",
      "Feedback\n",
      "About Us\n",
      "\n",
      "Search\n",
      "City or place…\n",
      "Copyright © 2024 «World-Weather.info» All rights reserved. Privacy policy\n",
      "Temperature units\n",
      "°F°C\n",
      "Icons\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res.json()[\"results\"][0][\"raw_content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so this is how we use the Tavily API to search the web, now let's implement this logic within a function which we can then use as a tool (ie action) for our ReAct agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str):\n",
    "    \"\"\"Use this tool to search the web for information.\"\"\"\n",
    "    # first we need to search the web for the query\n",
    "    res = requests.post(\n",
    "        f\"{tavily_url}/search\",\n",
    "        json={\n",
    "            \"api_key\": TAVILY_API_KEY,\n",
    "            \"query\": query\n",
    "        },\n",
    "    )\n",
    "    # now get all the URLs from the search results\n",
    "    urls = [x[\"url\"] for x in res.json()[\"results\"]]\n",
    "    # now extract the information from the URLs\n",
    "    res = requests.post(\n",
    "        f\"{tavily_url}/extract\",\n",
    "        json={\n",
    "            \"api_key\": TAVILY_API_KEY,\n",
    "            \"urls\": urls\n",
    "        },\n",
    "    )\n",
    "    # we return just the top result as otherwise we overload our LLM\n",
    "    return res.json()[\"results\"][0][\"raw_content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March 2024 Weather History in Tokyo Japan\n",
      "The data for this report comes from the Tokyo International Airport. See all nearby weather stations\n",
      "This report shows the past weather for Tokyo, providing a weather history for March 2024. It features all historical weather data series we have available, including the Tokyo temperature history for March 2024. You can drill down from year to month and even day level reports by clicking on the graphs.\n",
      "Tokyo Temperature History March 2024\n",
      "Hourly Temperature in March 2024 in Tokyo\n",
      "Compare Tokyo to another city:\n",
      "Cloud Cover in March 2024 in Tokyo\n",
      "Observed Weather in March 2024 in Tokyo\n",
      "Hours of Daylight and Twilight in March 2024 in Tokyo\n",
      "Sunrise & Sunset with Twilight in March 2024 in Tokyo\n",
      "Solar Elevation and Azimuth in March 2024 in Tokyo\n",
      "Moon Rise, Set & Phases in March 2024 in Tokyo\n",
      "Humidity Comfort Levels in March 2024 in Tokyo\n",
      "Wind Speed in March 2024 in Tokyo\n",
      "Hourly Wind Speed in March 2024 in Tokyo\n",
      "Hourly Wind Direction in 2024 in Tokyo\n",
      "Atmospheric Pressure in March 2024 in Tokyo\n",
      "Data Sources\n",
      "The details of the data sources used for this report can be found on the Tokyo International Airport page.\n",
      "See all nearby weather stations\n",
      "Disclaimer\n",
      "The information on this site is provided as is, without any assurances as to its accuracy or suitability for any purpose. Weather data is prone to errors, outages, and other defects. We assume no responsibility for any decisions made on the basis of the content presented on this site.\n",
      "We draw particular cautious attention to our reliance on the MERRA-2 model-based reconstructions for a number of important data series. While having the tremendous advantages of temporal and spatial completeness, these reconstructions: (1) are based on computer models that may have model-based errors, (2) are coarsely sampled on a 50 km grid and are therefore unable to reconstruct the local variations of many microclimates, and (3) have particular difficulty with the weather in some coastal areas, especially small islands.\n",
      "We further caution that our travel scores are only as good as the data that underpin them, that weather conditions at any given location and time are unpredictable and variable, and that the definition of the scores reflects a particular set of preferences that may not agree with those of any particular reader.\n",
      "Please review our full terms contained on our Terms of Service page.\n",
      "Language\n",
      "© Cedar Lake Ventures, Inc.\n",
      "You're permitted to use this graph as long as you provide prominent attribution with a link back close to the use of the graph. For example:\n",
      "© WeatherSpark.com\n",
      "For print usage, please acquire a license.\n"
     ]
    }
   ],
   "source": [
    "print(search(query=\"What is the weather in Tokyo?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's define two more tools...\n",
    "\n",
    "We will create a simple \"current date and time tool\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def date():\n",
    "    \"\"\"Use this tool to get the current date and time.\"\"\"\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return f\"The current date and time is {now}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And very importantly, we will define a tool that will be triggered when our LLM would like to provide it's final answer to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(text: str):\n",
    "    \"\"\"Use this tool to provide your final answer to the user.\"\"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate an additional part to our `system_prompt` to explain which tools are available to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'search',\n",
       "  'description': 'Use this tool to search the web for information.',\n",
       "  'args': {'query': 'str'}},\n",
       " {'name': 'date',\n",
       "  'description': 'Use this tool to get the current date and time.',\n",
       "  'args': {}},\n",
       " {'name': 'answer',\n",
       "  'description': 'Use this tool to provide your final answer to the user.',\n",
       "  'args': {'text': 'str'}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "# we get the various parameters/description from each tool function\n",
    "tools = [search, date, answer]\n",
    "tool_descriptions = [\n",
    "    {\n",
    "        \"name\": tool.__name__,\n",
    "        \"description\": str(inspect.getdoc(tool)),\n",
    "        \"args\": {\n",
    "            k: str(v).split(\": \")[1] for k, v in inspect.signature(tool).parameters.items()\n",
    "        }\n",
    "    }\n",
    "    for tool in tools\n",
    "]\n",
    "tool_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's parse these into text instructions that can be added to our `system_prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have access to the following tools ONLY, no other tools exist:\n",
      "\n",
      "{'name': 'search', 'description': 'Use this tool to search the web for information.', 'args': {'query': 'str'}}\n",
      "{'name': 'date', 'description': 'Use this tool to get the current date and time.', 'args': {}}\n",
      "{'name': 'answer', 'description': 'Use this tool to provide your final answer to the user.', 'args': {'text': 'str'}}\n"
     ]
    }
   ],
   "source": [
    "tool_instructions = (\n",
    "    \"You have access to the following tools ONLY, no other tools exist:\\n\\n\"\n",
    "    + \"\\n\".join([str(x) for x in tool_descriptions])\n",
    ")\n",
    "print(tool_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try calling our LLM again with these additional instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"thought\": \"I need to find out what Ollama refers to in the context of AI\",\n",
      "  \"action\": {\n",
      "    \"tool\": \"search\",\n",
      "    \"args\": {\"query\": \"Ollama AI\"}\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "res = ollama.chat(\n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"{system_prompt}\\n\\n{tool_instructions}\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Ollama in the context of AI?\"},\n",
    "    ],\n",
    "    format=\"json\",\n",
    "    options={\"temperature\": 0.0}\n",
    ")\n",
    "\n",
    "step = res[\"message\"][\"content\"]\n",
    "print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Our LLM has correctly generated the query we need. We can now parse this and pass it into the `search` tool as specified by our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama Explained: Transforming AI Accessibility and Language Processing\n",
      "In the rapidly evolving landscape of artificial intelligence (AI), accessibility and innovation are paramount. Among the myriad platforms and tools emerging in this space, one name stands out: Ollama. But what exactly is Ollama, and why is it garnering attention in the AI community? This article delves into the intricacies of Ollama, its methodologies, its potential impact on AI applications, and what this could mean for the future of human-machine interaction.\n",
      "Table of Content\n",
      "Understanding Ollama\n",
      "Ollama stands for (Omni-Layer Learning Language Acquisition Model), a novel approach to machine learning that promises to redefine how we perceive language acquisition and natural language processing.\n",
      "At its core, Ollama is a groundbreaking platform that democratizes access to large language models (LLMs) by enabling users to run them locally on their machines. Developed with a vision to empower individuals and organizations, Ollama provides a user-friendly interface and seamless integration capabilities, making it easier than ever to leverage the power of LLMs for various applications and use cases.\n",
      "The Genesis of Ollama:\n",
      "Traditional machine learning models have been successful in various linguistic tasks but often require extensive data labelling and preprocessing to function effectively. Ollama emerges as a paradigm shift, utilizing unsupervised learning techniques combined with deep neural networks that enable it to learn language structures without explicit grammatical rules or annotations.\n",
      "Understanding the Ollama Framework:\n",
      "OLLAMA’s architecture comprises multiple layers where each successive layer learns different linguistic patterns and abstract representations of speech. This multi-layered approach allows OLLAMA to progress from understanding basic sounds to grasping complex sentence structures, all without direct human intervention for labelling or structuring the input data.\n",
      "Key Features of Ollama\n",
      "Stepwise Guide to start Ollama\n",
      "Prerequisites:\n",
      "Step 1: Download Ollama\n",
      "Step 2: Install Ollama\n",
      "Step 3: Pull Your First Model (Optional)\n",
      "Replace ‘gemma’ with the specific model name if desired\n",
      "The Ollama library curates a diverse collection of LLMs, each with unique strengths and sizes. Some example are as follows:\n",
      "Step 4: Run and Use the Model\n",
      "Output for command “ollama run phi3”:\n",
      "ollama run phi3\n",
      "Managing Your LLM Ecosystem with the Ollama CLI\n",
      "The Ollama command-line interface (CLI) provides a range of functionalities to manage your LLM collection:\n",
      "The ollama run command is your gateway to interacting with any model on your machine. Need a quick summary of a text file? Pass it through an LLM and let it do the work. Ollama even supports multimodal models that can analyze images alongside text.\n",
      "We can also use ollama using python code as follows:\n",
      "Output:\n",
      "Applications of Ollama\n",
      "Ethical Considerations and Responsible AI\n",
      "While the potential of Ollama is vast and promising, it’s essential to address ethical considerations and ensure responsible AI practices. From mitigating bias and ensuring fairness to prioritizing privacy, transparency, and human oversight, developers and organizations must navigate these challenges to harness the full potential of Ollama while minimizing risks and promoting societal benefit.\n",
      "Conclusion\n",
      "Ollama’s revolutionary approach to natural language understanding heralds a new era where AI can learn and interpret human language as effortlessly as a child does. As researchers continue to refine this innovative model, we stand on the brink of witnessing an unprecedented leap in machine intelligence that could reshape our digital world.\n",
      "As AI technology continues to evolve, Ollama is poised to play a pivotal role in shaping its future development and deployment. With ongoing advancements in model capabilities, hardware optimization, decentralized model sharing, user experiences, and ethical AI frameworks, Ollama remains at the forefront of AI innovation, driving progress and democratization across all sectors of society.\n",
      "Please Login to comment...\n",
      "Similar Reads\n",
      "What kind of Experience do you want to share?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tool_choice = json.loads(res[\"message\"][\"content\"])[\"action\"][\"tool\"]\n",
    "args = json.loads(res[\"message\"][\"content\"])[\"action\"][\"args\"]\n",
    "\n",
    "# we use a dictionary to map the tool name to the tool function\n",
    "tool_selector = {x.__name__: x for x in tools}\n",
    "\n",
    "# now we select the tool and call it with the arguments\n",
    "observation = tool_selector[tool_choice](**args)\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Within the ReAct framework we would then pass this information back to our LLM via a new `observation` variable. Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"thought\": \"I need to find out what Ollama refers to in the context of AI\",\n",
      "  \"action\": {\n",
      "    \"tool\": \"search\",\n",
      "    \"args\": {\n",
      "      \"query\": \"Ollama AI\"\n",
      "    }\n",
      "  },\n",
      "  \"observation\": \"Ollama Explained: Transforming AI Accessibility and Language Processing\\nIn the rapidly evolving landscape of artificial intelligence (AI), accessibility and innovation are paramount. Among the myriad platforms and tools emerging in this space, one name stands out: Ollama. But what exactly is Ollama, and why is it garnering attention in the AI community? This article delves into the intricacies of Ollama, its methodologies, its potential impact on AI applications, and what this could mean for the future of human-machine interaction.\\nTable of Content\\nUnderstanding Ollama\\nOllama stands for (Omni-Layer Learning Language Acquisition Model), a novel approach to machine learning that promises to redefine how we perceive language acquisition and natural language processing.\\nAt its core, Ollama is a groundbreaking platform that democratizes access to large language models (LLMs) by enabling users to run them locally on their machines. Developed with a vision to empower individuals and organizations, Ollama provides a user-friendly interface and seamless integration capabilities, making it easier than ever to leverage the power of LLMs for various applications and use cases.\\nThe Genesis of Ollama:\\nTraditional machine learning models have been successful in various linguistic tasks but often require extensive data labelling and preprocessing to function effectively. Ollama emerges as a paradigm shift, utilizing unsupervised learning techniques combined with deep neural networks that enable it to learn language structures without explicit grammatical rules or annotations.\\nUnderstanding the Ollama Framework:\\nOLLAMA\\u2019s architecture comprises multiple layers where each successive layer learns different linguistic patterns and abstract representations of speech. This multi-layered approach allows OLLAMA to progress from understanding basic sounds to grasping complex sentence structures, all without direct human intervention for labelling or structuring the input data.\\nKey Features of Ollama\\nStepwise Guide to start Ollama\\nPrerequisites:\\nStep 1: Download Ollama\\nStep 2: Install Ollama\\nStep 3: Pull Your First Model (Optional)\\nReplace \\u2018gemma\\u2019 with the specific model name if desired\\nThe Ollama library curates a diverse collection of LLMs, each with unique strengths and sizes. Some example are as follows:\\nStep 4: Run and Use the Model\\nOutput for command \\u201collama run phi3\\u201d:\\nollama run phi3\\nManaging Your LLM Ecosystem with the Ollama CLI\\nThe Ollama command-line interface (CLI) provides a range of functionalities to manage your LLM collection:\\nThe ollama run command is your gateway to interacting with any model on your machine. Need a quick summary of a text file? Pass it through an LLM and let it do the work. Ollama even supports multimodal models that can analyze images alongside text.\\nWe can also use ollama using python code as follows:\\nOutput:\\nApplications of Ollama\\nEthical Considerations and Responsible AI\\nWhile the potential of Ollama is vast and promising, it\\u2019s essential to address ethical considerations and ensure responsible AI practices. From mitigating bias and ensuring fairness to prioritizing privacy, transparency, and human oversight, developers and organizations must navigate these challenges to harness the full potential of Ollama while minimizing risks and promoting societal benefit.\\nConclusion\\nOllama\\u2019s revolutionary approach to natural language understanding heralds a new era where AI can learn and interpret human language as effortlessly as a child does. As researchers continue to refine this innovative model, we stand on the brink of witnessing an unprecedented leap in machine intelligence that could reshape our digital world.\\nAs AI technology continues to evolve, Ollama is poised to play a pivotal role in shaping its future development and deployment. With ongoing advancements in model capabilities, hardware optimization, decentralized model sharing, user experiences, and ethical AI frameworks, Ollama remains at the forefront of AI innovation, driving progress and democratization across all sectors of society.\\nPlease Login to comment...\\nSimilar Reads\\nWhat kind of Experience do you want to share?\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "iteration = json.loads(step)\n",
    "iteration[\"observation\"] = observation\n",
    "iteration_str = json.dumps(iteration, indent=2)\n",
    "print(iteration_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed this back into our chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"thought\": \"I need to summarize what Ollama is in simple terms\", \"action\": {\"tool\": \"answer\", \"args\": {\"text\": \"Ollama is a platform that enables users to run large language models locally on their machines, making it easier to leverage AI for various applications and use cases.\"}} }\n"
     ]
    }
   ],
   "source": [
    "res = ollama.chat(\n",
    "    model=\"llama3.2:3b-instruct-fp16\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"{system_prompt}\\n\\n{tool_instructions}\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What is Ollama in the context of AI?\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"Step 1:\\n{iteration_str}\\n\\nWhat do I do next to answer the user's question...\"},\n",
    "    ],\n",
    "    format=\"json\",\n",
    "    options={\"temperature\": 0.0}\n",
    ")\n",
    "\n",
    "step2 = res[\"message\"][\"content\"]\n",
    "print(step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! Our final answer from the LLM is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is a platform that enables users to run large language models locally on their machines, making it easier to leverage AI for various applications and use cases.\n"
     ]
    }
   ],
   "source": [
    "step2_json = json.loads(step2)\n",
    "print(step2_json[\"action\"][\"args\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, now let's take everything we've done so far and use it to construct our ReAct agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "class ReActAgent:\n",
    "    def __init__(self, tools: list[Callable]):\n",
    "        self.messages = []\n",
    "        self.tools = {x.__name__: x for x in tools}\n",
    "        tool_instructions = self._format_tool_instructions(tools=tools)\n",
    "        self.system_prompt = system_prompt\n",
    "        # add system prompt and tool instructions to our messages\n",
    "        self.messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"{self.system_prompt}\\n\\n{tool_instructions}\"\n",
    "        })\n",
    "\n",
    "    def _format_tool_instructions(self, tools: list[Callable]):\n",
    "        # get the various parameters/description from each tool function\n",
    "        tool_descriptions = [\n",
    "            {\n",
    "                \"name\": tool.__name__,\n",
    "                \"description\": str(inspect.getdoc(tool)),\n",
    "                \"args\": {\n",
    "                    k: str(v).split(\": \")[1] for k, v in inspect.signature(tool).parameters.items()\n",
    "                }\n",
    "            } for tool in tools\n",
    "        ]\n",
    "        # parse these into text instructions that can be added to our system prompt\n",
    "        tool_instructions = (\n",
    "            \"You have access to the following tools ONLY, no other tools exist:\\n\\n\"\n",
    "            + \"\\n\".join([str(x) for x in tool_descriptions])\n",
    "        )\n",
    "        return tool_instructions\n",
    "\n",
    "    def __call__(self, prompt: str, max_steps: int = 10):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        step_count = 1\n",
    "        steps = []\n",
    "        while step_count < max_steps:\n",
    "            # get the next step\n",
    "            step_dict = self._call_llm(\n",
    "                messages=self.messages + (\n",
    "                    [self._format_scratchpad(steps)] if steps else []\n",
    "                )\n",
    "            )\n",
    "            try:\n",
    "                # get the tool choice and arguments\n",
    "                tool_choice = step_dict[\"action\"][\"tool\"]\n",
    "                args = step_dict[\"action\"][\"args\"]\n",
    "            except KeyError as e:\n",
    "                print(f\"! Handled error: {e}\")\n",
    "                # in case we get error, return content direct\n",
    "                step_dict = {\n",
    "                    \"thought\": step_dict.get(\"thought\", \"No thought provided\"),\n",
    "                    \"action\": step_dict.get(\"action\", {\n",
    "                        \"tool\": \"answer\",\n",
    "                        \"args\": {\"text\": str(step_dict)}\n",
    "                    }),\n",
    "                }\n",
    "                tool_choice = step_dict[\"action\"][\"tool\"]\n",
    "                args = step_dict[\"action\"][\"args\"]\n",
    "            self._print_react(step_count=step_count, step_dict=step_dict)\n",
    "            if tool_choice == \"answer\":\n",
    "                # we've reached the final step\n",
    "                return step_dict[\"action\"][\"args\"][\"text\"]\n",
    "            else:\n",
    "                # otherwise we call the chosen tool\n",
    "                observation = self.tools[tool_choice](**args)\n",
    "                print(f\"Observation {step_count}: {observation[:200]}...\")\n",
    "            # add the step to our scratchpad\n",
    "            steps.append({\n",
    "                \"thought\": step_dict[\"thought\"],\n",
    "                \"action\": step_dict[\"action\"],\n",
    "                \"observation\": observation\n",
    "            })\n",
    "            step_count += 1\n",
    "        # if we get here we've hit the max steps so we force the answer tool\n",
    "        # to do this we modify the system prompt to only show the answer tool\n",
    "        messages = self.messages.copy()\n",
    "        tool_instructions = self._format_tool_instructions(tools=[self.tools[\"answer\"]])\n",
    "        messages[0][\"content\"] = f\"{self.system_prompt}\\n\\n{tool_instructions}\"\n",
    "        # now we call the LLM with the modified system prompt\n",
    "        step_dict = self._call_llm(messages=messages)\n",
    "        return step_dict[\"action\"][\"args\"][\"text\"]\n",
    "\n",
    "    def _call_llm(self, messages: list[dict]) -> dict:\n",
    "        res = ollama.chat(\n",
    "            model=\"llama3.2:3b-instruct-fp16\",\n",
    "            messages=messages,\n",
    "            format=\"json\",\n",
    "            options={\"temperature\": 0.0},\n",
    "        )\n",
    "        step_dict = json.loads(res[\"message\"][\"content\"])\n",
    "        return step_dict\n",
    "        \n",
    "    def _format_scratchpad(self, steps: list[dict]) -> dict:\n",
    "        steps_str = \"\"\n",
    "        for i, step in enumerate(steps):\n",
    "            steps_str += f\"Step {i+1}:\\n{json.dumps(step, indent=2)}\\n\\n\"\n",
    "        steps_str += \"What do I do next to answer the user's question...\"\n",
    "        return {\"role\": \"assistant\", \"content\": steps_str}\n",
    "\n",
    "    def _print_react(self, step_count: int, step_dict: dict) -> None:\n",
    "        \"\"\"Prints the Reasoning (thought) and Action step\"\"\"\n",
    "        react = \"\\n\".join([\n",
    "            f\"Thought {step_count}: {step_dict['thought']}\",\n",
    "            f\"Action {step_count}: {step_dict['action']}\",\n",
    "        ])\n",
    "        print(react)\n",
    "\n",
    "agent = ReActAgent(tools=[search, date, answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1: I need to find out what Ollama refers to in the context of AI\n",
      "Action 1: {'tool': 'search', 'args': {'query': 'Ollama AI'}}\n",
      "Observation 1: Ollama Explained: Transforming AI Accessibility and Language Processing\n",
      "In the rapidly evolving landscape of artificial intelligence (AI), accessibility and innovation are paramount. Among the myriad ...\n",
      "Thought 2: I need to summarize what Ollama is in simple terms\n",
      "Action 2: {'tool': 'answer', 'args': {'text': 'Ollama is a platform that enables users to run large language models locally on their machines, making it easier to leverage AI for various applications and use cases.'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ollama is a platform that enables users to run large language models locally on their machines, making it easier to leverage AI for various applications and use cases.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"What is Ollama in the context of AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin a new conversation, we must reinitialize our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1: I need to find out the current date and time\n",
      "Action 1: {'tool': 'date', 'args': {}}\n",
      "Observation 1: The current date and time is 2024-11-03 21:46:43...\n",
      "! Handled error: 'action'\n",
      "Thought 2: No thought provided\n",
      "Action 2: {'tool': 'answer', 'args': {'text': '{}'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{}'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = ReActAgent(tools=[search, date, answer])\n",
    "\n",
    "agent(\"What time is it right now?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
