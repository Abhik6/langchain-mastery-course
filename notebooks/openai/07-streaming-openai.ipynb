{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain Essentials Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming With Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
    "\n",
    "In this example, we will introduce LangChain's Async Streaming, collecting data gradually and understanding how it will work. We'll provide examples for both OpenAI's `gpt-4o-mini` *and* Meta's `llama3.2` via Ollama!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, please see the [Ollama version](https://github.com/aurelio-labs/langchain-course/blob/main/notebooks/ollama/06-lcel-ollama.ipynb) of this example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the LLM, we'll start by initializing our connection to the OpenAI API. We do need an OpenAI API key, which you can get from the [OpenAI platform](https://platform.openai.com/api-keys).\n",
    "\n",
    "We will use the `gpt-4o-mini` model with a `temperature` of `0.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
    "    or getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-3e8340ed-f353-4bf9-b3b0-12431bb6e993-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_out = llm.invoke(\"Hello there\")\n",
    "llm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by creating a aysnc stream from our LLM, we will do this within a for loop so that we can print each chunk as it is being generated. Whilst we do this we are also printing a pipe or '|' to the end of each chunk, and by setting 'flush' equal to True, it ensures that the output is immediately written to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|N|LP| stands| for| Natural| Language| Processing|.| It| is| a| field| of| artificial| intelligence| (|AI|)| that| focuses| on| the| interaction| between| computers| and| humans| through| natural| language|.| The| goal| of| NLP| is| to| enable| computers| to| understand|,| interpret|,| and| generate| human| language| in| a| way| that| is| both| meaningful| and| useful|.| This| involves| various| tasks| such| as| language| translation|,| sentiment| analysis|,| text| summar|ization|,| speech| recognition|,| and| more|.| NLP| combines| techniques| from| lingu|istics|,| computer| science|,| and| machine| learning| to| process| and| analyze| large| amounts| of| natural| language| data|.||"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "async for chunk in llm.astream(\"What does NLP mean?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we appended each chunk to the 'chunks' list, we can also see what is inside each and every chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-5763c463-7173-429a-8d35-b26af354a507')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=\"Hello! I'm an\", additional_kwargs={}, response_metadata={}, id='run-5763c463-7173-429a-8d35-b26af354a507')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content=' stands', additional_kwargs={}, response_metadata={}, id='run-1893abaf-281c-4ce5-9c92-112c8064c3eb')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0] + chunks[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make some tools to use on a async agent executor to show that functions do not stream the process and only the output, meaning if you are doing an entire LLM search within these functions, the entire output will be shown rather then chunks on an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x ** y\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
    "    return y - x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a prompt to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you're a helpful assistant\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generating outputs for the tools to the agent, this is also where we connect the llm and the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "\n",
    "tools = [add, multiply, exponentiate, subtract]\n",
    "\n",
    "agent = create_tool_calling_agent(\n",
    "    llm=llm, tools=tools, prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to connect the tool exectutional functionality to the overseer agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the astream function as opposed to the invoke function, this allows the output of the function to be shown to us as chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'actions': [...], 'messages': [...]}\n",
      "------\n",
      "{'messages': [...], 'steps': [...]}\n",
      "------\n",
      "{'messages': [...],\n",
      " 'output': \"Let's break down the calculations step by step:\\n\"\n",
      "           '\\n'\n",
      "           '1. **5 + 5 = 10**\\n'\n",
      "           '2. **10 * 10 = 100**\\n'\n",
      "           '3. **10 to the power of 3 = 1000**\\n'\n",
      "           '4. **1000 - 99 = 901**\\n'\n",
      "           '\\n'\n",
      "           'So, the final result is **901**.'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "chunks = []\n",
    "\n",
    "async for chunk in agent_executor.astream(\n",
    "    {\"input\": \"in the following order, what is 5+5, * 10, to the power of 3, minus 99?\"}\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "    print(\"------\")\n",
    "    pprint.pprint(chunk, depth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside of each chunk we can see different attributes credited to each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ToolAgentAction(tool='add', tool_input={'x': 5, 'y': 5}, log=\"\\nInvoking: `add` with `{'x': 5, 'y': 5}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'function': {'arguments': '{\"x\": 5, \"y\": 5}', 'name': 'add'}, 'type': 'function'}, {'index': 1, 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'function': {'arguments': '{\"x\": 10, \"y\": 10}', 'name': 'multiply'}, 'type': 'function'}, {'index': 2, 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'function': {'arguments': '{\"x\": 10, \"y\": 3}', 'name': 'exponentiate'}, 'type': 'function'}, {'index': 3, 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'function': {'arguments': '{\"x\": 99, \"y\": 99}', 'name': 'subtract'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-0a931944-a303-48a2-9cbe-dfe323593d0c', tool_calls=[{'name': 'add', 'args': {'x': 5, 'y': 5}, 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'x': 10, 'y': 10}, 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'type': 'tool_call'}, {'name': 'exponentiate', 'args': {'x': 10, 'y': 3}, 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'type': 'tool_call'}, {'name': 'subtract', 'args': {'x': 99, 'y': 99}, 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'add', 'args': '{\"x\": 5, \"y\": 5}', 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'index': 0, 'type': 'tool_call_chunk'}, {'name': 'multiply', 'args': '{\"x\": 10, \"y\": 10}', 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'index': 1, 'type': 'tool_call_chunk'}, {'name': 'exponentiate', 'args': '{\"x\": 10, \"y\": 3}', 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'index': 2, 'type': 'tool_call_chunk'}, {'name': 'subtract', 'args': '{\"x\": 99, \"y\": 99}', 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'index': 3, 'type': 'tool_call_chunk'}])], tool_call_id='call_5LY91QynPiCypmkWL6t95m8p')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0][\"actions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the 'messages' attribute we can see the content of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'function': {'arguments': '{\"x\": 5, \"y\": 5}', 'name': 'add'}, 'type': 'function'}, {'index': 1, 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'function': {'arguments': '{\"x\": 10, \"y\": 10}', 'name': 'multiply'}, 'type': 'function'}, {'index': 2, 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'function': {'arguments': '{\"x\": 10, \"y\": 3}', 'name': 'exponentiate'}, 'type': 'function'}, {'index': 3, 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'function': {'arguments': '{\"x\": 99, \"y\": 99}', 'name': 'subtract'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-0a931944-a303-48a2-9cbe-dfe323593d0c', tool_calls=[{'name': 'add', 'args': {'x': 5, 'y': 5}, 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'x': 10, 'y': 10}, 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'type': 'tool_call'}, {'name': 'exponentiate', 'args': {'x': 10, 'y': 3}, 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'type': 'tool_call'}, {'name': 'subtract', 'args': {'x': 99, 'y': 99}, 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'add', 'args': '{\"x\": 5, \"y\": 5}', 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'index': 0, 'type': 'tool_call_chunk'}, {'name': 'multiply', 'args': '{\"x\": 10, \"y\": 10}', 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'index': 1, 'type': 'tool_call_chunk'}, {'name': 'exponentiate', 'args': '{\"x\": 10, \"y\": 3}', 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'index': 2, 'type': 'tool_call_chunk'}, {'name': 'subtract', 'args': '{\"x\": 99, \"y\": 99}', 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'index': 3, 'type': 'tool_call_chunk'}])]\n",
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'function': {'arguments': '{\"x\": 5, \"y\": 5}', 'name': 'add'}, 'type': 'function'}, {'index': 1, 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'function': {'arguments': '{\"x\": 10, \"y\": 10}', 'name': 'multiply'}, 'type': 'function'}, {'index': 2, 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'function': {'arguments': '{\"x\": 10, \"y\": 3}', 'name': 'exponentiate'}, 'type': 'function'}, {'index': 3, 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'function': {'arguments': '{\"x\": 99, \"y\": 99}', 'name': 'subtract'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-0a931944-a303-48a2-9cbe-dfe323593d0c', tool_calls=[{'name': 'add', 'args': {'x': 5, 'y': 5}, 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'x': 10, 'y': 10}, 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'type': 'tool_call'}, {'name': 'exponentiate', 'args': {'x': 10, 'y': 3}, 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'type': 'tool_call'}, {'name': 'subtract', 'args': {'x': 99, 'y': 99}, 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'add', 'args': '{\"x\": 5, \"y\": 5}', 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'index': 0, 'type': 'tool_call_chunk'}, {'name': 'multiply', 'args': '{\"x\": 10, \"y\": 10}', 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'index': 1, 'type': 'tool_call_chunk'}, {'name': 'exponentiate', 'args': '{\"x\": 10, \"y\": 3}', 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'index': 2, 'type': 'tool_call_chunk'}, {'name': 'subtract', 'args': '{\"x\": 99, \"y\": 99}', 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'index': 3, 'type': 'tool_call_chunk'}])]\n",
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'function': {'arguments': '{\"x\": 5, \"y\": 5}', 'name': 'add'}, 'type': 'function'}, {'index': 1, 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'function': {'arguments': '{\"x\": 10, \"y\": 10}', 'name': 'multiply'}, 'type': 'function'}, {'index': 2, 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'function': {'arguments': '{\"x\": 10, \"y\": 3}', 'name': 'exponentiate'}, 'type': 'function'}, {'index': 3, 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'function': {'arguments': '{\"x\": 99, \"y\": 99}', 'name': 'subtract'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-0a931944-a303-48a2-9cbe-dfe323593d0c', tool_calls=[{'name': 'add', 'args': {'x': 5, 'y': 5}, 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'x': 10, 'y': 10}, 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'type': 'tool_call'}, {'name': 'exponentiate', 'args': {'x': 10, 'y': 3}, 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'type': 'tool_call'}, {'name': 'subtract', 'args': {'x': 99, 'y': 99}, 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'add', 'args': '{\"x\": 5, \"y\": 5}', 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'index': 0, 'type': 'tool_call_chunk'}, {'name': 'multiply', 'args': '{\"x\": 10, \"y\": 10}', 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'index': 1, 'type': 'tool_call_chunk'}, {'name': 'exponentiate', 'args': '{\"x\": 10, \"y\": 3}', 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'index': 2, 'type': 'tool_call_chunk'}, {'name': 'subtract', 'args': '{\"x\": 99, \"y\": 99}', 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'index': 3, 'type': 'tool_call_chunk'}])]\n",
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'function': {'arguments': '{\"x\": 5, \"y\": 5}', 'name': 'add'}, 'type': 'function'}, {'index': 1, 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'function': {'arguments': '{\"x\": 10, \"y\": 10}', 'name': 'multiply'}, 'type': 'function'}, {'index': 2, 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'function': {'arguments': '{\"x\": 10, \"y\": 3}', 'name': 'exponentiate'}, 'type': 'function'}, {'index': 3, 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'function': {'arguments': '{\"x\": 99, \"y\": 99}', 'name': 'subtract'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-0a931944-a303-48a2-9cbe-dfe323593d0c', tool_calls=[{'name': 'add', 'args': {'x': 5, 'y': 5}, 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'x': 10, 'y': 10}, 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'type': 'tool_call'}, {'name': 'exponentiate', 'args': {'x': 10, 'y': 3}, 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'type': 'tool_call'}, {'name': 'subtract', 'args': {'x': 99, 'y': 99}, 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'add', 'args': '{\"x\": 5, \"y\": 5}', 'id': 'call_5LY91QynPiCypmkWL6t95m8p', 'index': 0, 'type': 'tool_call_chunk'}, {'name': 'multiply', 'args': '{\"x\": 10, \"y\": 10}', 'id': 'call_e7O8LAZf32zNU5ThM6uUMwx3', 'index': 1, 'type': 'tool_call_chunk'}, {'name': 'exponentiate', 'args': '{\"x\": 10, \"y\": 3}', 'id': 'call_ZVGzvK1x2egCZRAA08ewHPK7', 'index': 2, 'type': 'tool_call_chunk'}, {'name': 'subtract', 'args': '{\"x\": 99, \"y\": 99}', 'id': 'call_xO1zz7Jsl6PATzJW5ttMLh0O', 'index': 3, 'type': 'tool_call_chunk'}])]\n",
      "[FunctionMessage(content='10.0', additional_kwargs={}, response_metadata={}, name='add')]\n",
      "[FunctionMessage(content='100.0', additional_kwargs={}, response_metadata={}, name='multiply')]\n",
      "[FunctionMessage(content='1000.0', additional_kwargs={}, response_metadata={}, name='exponentiate')]\n",
      "[FunctionMessage(content='0.0', additional_kwargs={}, response_metadata={}, name='subtract')]\n",
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_afXmOj0CfVGjocBmTJSlnjNe', 'function': {'arguments': '{\"x\": 10, \"y\": 10}', 'name': 'multiply'}, 'type': 'function'}, {'index': 1, 'id': 'call_dG17BU29QjFAmIfuFE8O17aL', 'function': {'arguments': '{\"x\": 10, \"y\": 3}', 'name': 'exponentiate'}, 'type': 'function'}, {'index': 2, 'id': 'call_SsjDbPC9LlFcLBUUFkNCepTT', 'function': {'arguments': '{\"x\": 1000, \"y\": 99}', 'name': 'subtract'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-1c5819ba-b4e4-48c8-ad01-57f698d60edf', tool_calls=[{'name': 'multiply', 'args': {'x': 10, 'y': 10}, 'id': 'call_afXmOj0CfVGjocBmTJSlnjNe', 'type': 'tool_call'}, {'name': 'exponentiate', 'args': {'x': 10, 'y': 3}, 'id': 'call_dG17BU29QjFAmIfuFE8O17aL', 'type': 'tool_call'}, {'name': 'subtract', 'args': {'x': 1000, 'y': 99}, 'id': 'call_SsjDbPC9LlFcLBUUFkNCepTT', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'multiply', 'args': '{\"x\": 10, \"y\": 10}', 'id': 'call_afXmOj0CfVGjocBmTJSlnjNe', 'index': 0, 'type': 'tool_call_chunk'}, {'name': 'exponentiate', 'args': '{\"x\": 10, \"y\": 3}', 'id': 'call_dG17BU29QjFAmIfuFE8O17aL', 'index': 1, 'type': 'tool_call_chunk'}, {'name': 'subtract', 'args': '{\"x\": 1000, \"y\": 99}', 'id': 'call_SsjDbPC9LlFcLBUUFkNCepTT', 'index': 2, 'type': 'tool_call_chunk'}])]\n",
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_afXmOj0CfVGjocBmTJSlnjNe', 'function': {'arguments': '{\"x\": 10, \"y\": 10}', 'name': 'multiply'}, 'type': 'function'}, {'index': 1, 'id': 'call_dG17BU29QjFAmIfuFE8O17aL', 'function': {'arguments': '{\"x\": 10, \"y\": 3}', 'name': 'exponentiate'}, 'type': 'function'}, {'index': 2, 'id': 'call_SsjDbPC9LlFcLBUUFkNCepTT', 'function': {'arguments': '{\"x\": 1000, \"y\": 99}', 'name': 'subtract'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-1c5819ba-b4e4-48c8-ad01-57f698d60edf', tool_calls=[{'name': 'multiply', 'args': {'x': 10, 'y': 10}, 'id': 'call_afXmOj0CfVGjocBmTJSlnjNe', 'type': 'tool_call'}, {'name': 'exponentiate', 'args': {'x': 10, 'y': 3}, 'id': 'call_dG17BU29QjFAmIfuFE8O17aL', 'type': 'tool_call'}, {'name': 'subtract', 'args': {'x': 1000, 'y': 99}, 'id': 'call_SsjDbPC9LlFcLBUUFkNCepTT', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'multiply', 'args': '{\"x\": 10, \"y\": 10}', 'id': 'call_afXmOj0CfVGjocBmTJSlnjNe', 'index': 0, 'type': 'tool_call_chunk'}, {'name': 'exponentiate', 'args': '{\"x\": 10, \"y\": 3}', 'id': 'call_dG17BU29QjFAmIfuFE8O17aL', 'index': 1, 'type': 'tool_call_chunk'}, {'name': 'subtract', 'args': '{\"x\": 1000, \"y\": 99}', 'id': 'call_SsjDbPC9LlFcLBUUFkNCepTT', 'index': 2, 'type': 'tool_call_chunk'}])]\n",
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_afXmOj0CfVGjocBmTJSlnjNe', 'function': {'arguments': '{\"x\": 10, \"y\": 10}', 'name': 'multiply'}, 'type': 'function'}, {'index': 1, 'id': 'call_dG17BU29QjFAmIfuFE8O17aL', 'function': {'arguments': '{\"x\": 10, \"y\": 3}', 'name': 'exponentiate'}, 'type': 'function'}, {'index': 2, 'id': 'call_SsjDbPC9LlFcLBUUFkNCepTT', 'function': {'arguments': '{\"x\": 1000, \"y\": 99}', 'name': 'subtract'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-1c5819ba-b4e4-48c8-ad01-57f698d60edf', tool_calls=[{'name': 'multiply', 'args': {'x': 10, 'y': 10}, 'id': 'call_afXmOj0CfVGjocBmTJSlnjNe', 'type': 'tool_call'}, {'name': 'exponentiate', 'args': {'x': 10, 'y': 3}, 'id': 'call_dG17BU29QjFAmIfuFE8O17aL', 'type': 'tool_call'}, {'name': 'subtract', 'args': {'x': 1000, 'y': 99}, 'id': 'call_SsjDbPC9LlFcLBUUFkNCepTT', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'multiply', 'args': '{\"x\": 10, \"y\": 10}', 'id': 'call_afXmOj0CfVGjocBmTJSlnjNe', 'index': 0, 'type': 'tool_call_chunk'}, {'name': 'exponentiate', 'args': '{\"x\": 10, \"y\": 3}', 'id': 'call_dG17BU29QjFAmIfuFE8O17aL', 'index': 1, 'type': 'tool_call_chunk'}, {'name': 'subtract', 'args': '{\"x\": 1000, \"y\": 99}', 'id': 'call_SsjDbPC9LlFcLBUUFkNCepTT', 'index': 2, 'type': 'tool_call_chunk'}])]\n",
      "[FunctionMessage(content='100.0', additional_kwargs={}, response_metadata={}, name='multiply')]\n",
      "[FunctionMessage(content='1000.0', additional_kwargs={}, response_metadata={}, name='exponentiate')]\n",
      "[FunctionMessage(content='-901.0', additional_kwargs={}, response_metadata={}, name='subtract')]\n",
      "[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_l6yTlDzejhqKtpXUpvwa88Eg', 'function': {'arguments': '{\"x\":1000,\"y\":99}', 'name': 'subtract'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63'}, id='run-01b5312f-6205-4abe-a609-a8451f5dab89', tool_calls=[{'name': 'subtract', 'args': {'x': 1000, 'y': 99}, 'id': 'call_l6yTlDzejhqKtpXUpvwa88Eg', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'subtract', 'args': '{\"x\":1000,\"y\":99}', 'id': 'call_l6yTlDzejhqKtpXUpvwa88Eg', 'index': 0, 'type': 'tool_call_chunk'}])]\n",
      "[FunctionMessage(content='-901.0', additional_kwargs={}, response_metadata={}, name='subtract')]\n",
      "[AIMessage(content=\"Let's break down the calculations step by step:\\n\\n1. **5 + 5 = 10**\\n2. **10 * 10 = 100**\\n3. **10 to the power of 3 = 1000**\\n4. **1000 - 99 = 901**\\n\\nSo, the final result is **901**.\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can also see the astream events in action, this shows the chunks within each function, when it starts, ends, produces an output etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content='Why' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content=' don' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content=\"'t\" additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content=' scientists' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content=' trust' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content=' atoms' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content='?\\n\\n' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content='Because' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content=' they' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content=' make' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content=' up' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content=' everything' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125'} id='run-53e12577-999c-491e-937f-26a9444a3f94'\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler\n",
    "from langchain_core.messages import HumanMessage, BaseMessage\n",
    "from langchain_core.outputs import LLMResult\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import uuid\n",
    "\n",
    "class MyCustomSyncHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"Sync handler being called in a `thread_pool_executor`: token: {token}\")\n",
    "\n",
    "\n",
    "class MyCustomAsyncHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Async callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    async def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "        await asyncio.sleep(0.3)\n",
    "        class_name = serialized[\"name\"]\n",
    "        yield \"Hi! I just woke up. Your llm is starting\"\n",
    "\n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "        await asyncio.sleep(0.3)\n",
    "        yield \"Hi! I just woke up. Your llm is ending\"\n",
    "\n",
    "\n",
    "# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n",
    "# Additionally, we pass in a list with our custom handler\n",
    "chat = ChatOpenAI(\n",
    "    max_tokens=25,\n",
    "    streaming=True,\n",
    "    callbacks=[MyCustomAsyncHandler()],\n",
    ")\n",
    "\n",
    "response = chat.astream([HumanMessage(content=\"Tell me a joke\")])\n",
    "\n",
    "async for token in response:\n",
    "    print(token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
