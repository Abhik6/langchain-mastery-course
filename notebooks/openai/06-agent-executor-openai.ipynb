{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain Essentials Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Agent Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we will continue from the [introduction to agents](https://aurelio.ai/learn/langchain-agents-intro) and dive deeper into agents. Learning how to build our custom agent execution loop for v0.3 of LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, please see the [Ollama version](https://github.com/aurelio-labs/langchain-course/blob/main/notebooks/ollama/05-agents-executor-ollama.ipynb) of this example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> ⚠️ If using LangSmith, add your API key below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
    "    getpass(\"Enter LangSmith API Key: \")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-agent-executor-openai\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Agent Executor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we talk about agents, a significant part of an \"agent\" is simple code logic,\n",
    "iteratively rerunning LLM calls and processing their output. The exact logic varies\n",
    "significantly, but one well-known example is the **ReAct** agent.\n",
    "\n",
    "![ReAct process](https://www.aurelio.ai/_next/image?url=%2Fimages%2Fposts%2Fai-agents%2Fai-agents-00.png&w=640&q=75)\n",
    "\n",
    "**Re**ason + **Act**ion (ReAct) agents use iterative _reasoning_ and _action_ steps to\n",
    "incorporate chain-of-thought and tool-use into their execution. During the _reasoning_\n",
    "step, the LLM generates the steps to take to answer the query. Next, the LLM generates\n",
    "the _action_ input, which our code logic parses into a tool call.\n",
    "\n",
    "![Agentic graph of ReAct](https://www.aurelio.ai/_next/image?url=%2Fimages%2Fposts%2Fai-agents%2Fai-agents-01.png&w=640&q=75)\n",
    "\n",
    "Following our action step, we get an observation from the tool call. Then, we feed the\n",
    "observation back into the agent executor logic for a final answer or further reasoning\n",
    "and action steps.\n",
    "\n",
    "The agent and agent executor we will be building will follow this pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the previous chapter, we will define a few tools to use within our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(x: float, y: float) -> float:\n",
    "    \"\"\"Add 'x' and 'y'.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "# Define the multiply tool\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply 'x' and 'y'.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "# Define the exponentiate tool\n",
    "@tool\n",
    "def exponentiate(x: float, y: float) -> float:\n",
    "    \"\"\"Raise 'x' to the power of 'y'.\"\"\"\n",
    "    return x ** y\n",
    "\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract 'x' from 'y'.\"\"\"\n",
    "    return y - x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `@tool` decorator our function is turned into a `StructuredTool` object, which we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='add', description=\"Add 'x' and 'y'.\", args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x10c1d8a40>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the tool name, description, and arg schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add.name='add'\n",
      "add.description=\"Add 'x' and 'y'.\"\n"
     ]
    }
   ],
   "source": [
    "print(f\"{add.name=}\\n{add.description=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Add 'x' and 'y'.\",\n",
       " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
       "  'y': {'title': 'Y', 'type': 'number'}},\n",
       " 'required': ['x', 'y'],\n",
       " 'title': 'add',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add.args_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `args_schema` is a pydantic model that is transformed into the JSON schema above and passed into our LLM, it is this that defines _how_ the tool is used for the LLM. We can see this with other tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"Raise 'x' to the power of 'y'.\",\n",
       " 'properties': {'x': {'title': 'X', 'type': 'number'},\n",
       "  'y': {'title': 'Y', 'type': 'number'}},\n",
       " 'required': ['x', 'y'],\n",
       " 'title': 'exponentiate',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponentiate.args_schema.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When invoking the tool, a JSON string output by the LLM will be parsed into JSON and then consumed as kwargs, similar to the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 5, 'y': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "llm_output_string = \"{\\\"x\\\": 5, \\\"y\\\": 2}\"  # this is the output from the LLM\n",
    "llm_output_dict = json.loads(llm_output_string)  # load as dictionary\n",
    "llm_output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is then passed into the tool function as `kwargs` (keyword arguments) as indicated by the `**` operator - the `**` operator is used to unpack the dictionary into keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponentiate.func(**llm_output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This covers the basics of tools and how they work, let's move on to creating the agent itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **L**ang**C**hain **E**pression **L**anguage (LCEL) to construct the agent. We will cover LCEL more in the next chapter, but for now - all we need to know is that our agent will be constructed using syntax and components like so:\n",
    "\n",
    "\n",
    "```\n",
    "agent = (\n",
    "    <input parameters, including chat history and user query>\n",
    "    | <prompt>\n",
    "    | <LLM with tools>\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need this agent to remember previous interactions within the conversation. To do that, we will use the `ChatPromptTemplate` with a system message, a placeholder for our chat history, a placeholder for the user query, and finally a placeholder for the agent scratchpad.\n",
    "\n",
    "The agent scratchpad is where the agent writes its notes as it works through multiple internal thought and tool-use steps to produce a final output for the user. This scratchpad is a list of messages with alternating roles of `ai` (for the tool call) and `tool` (for the tool execution output). Both message types require a `tool_call_id` field which is used to link the respective AI and tool messages - this can be required when we many tool calls happening in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You're a helpful assistant. When answering a user's question \"\n",
    "        \"you should first use one of the tools provided. After using a \"\n",
    "        \"tool the tool output will be provided in the \"\n",
    "        \"'scratchpad' below. If you have an answer in the \"\n",
    "        \"scratchpad you should not use any more tools and \"\n",
    "        \"instead answer directly to the user.\"\n",
    "    )),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must define our LLM, we will use the `gpt-4o-mini` model with a `temperature` of `0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
    "    or getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add tools to our LLM, we will use the `bind_tools` method within the LCEL constructor, which will take our tools and add them to the LLM. We'll also include the `tool_choice=\"any\"` argument to `bind_tools`, which tells the LLM that it _MUST_ use a tool, ie it cannot provide a final answer directly (in therefore not using a tool):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "\n",
    "tools = [add, subtract, multiply, exponentiate]\n",
    "\n",
    "# define the agent runnable\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We invoke the agent with the `invoke` method, passing in the input and chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_aTtpCLGu3FkDdNoKsKfTWmPh', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 197, 'total_tokens': 215, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4154b4dc-3efd-47ea-a986-f8c642c92777-0', tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': 'call_aTtpCLGu3FkDdNoKsKfTWmPh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 197, 'output_tokens': 18, 'total_tokens': 215, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = agent.invoke({\"input\": \"What is 10 + 10\", \"chat_history\": []})\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we set `tool_choice=\"any\"` to force the tool output, the usual `content` field will be empty as that field is used for natural language output, ie the _final answer_ of the LLM. To find our tool output, we need to look at the `tool_calls` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'x': 10, 'y': 10},\n",
       "  'id': 'call_aTtpCLGu3FkDdNoKsKfTWmPh',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we have the tool `name` that our LLM wants to use and the `args` that it\n",
    "wants to pass to that tool. We can see that the tool `add` is being used with the\n",
    "arguments `x=10` and `y=10`. The `agent.invoke` method has _not_ executed the tool\n",
    "function; we need to write that part of the agent code ourselves.\n",
    "\n",
    "Executing the tool code requires two steps:\n",
    "\n",
    "1. Map the tool `name` to the tool function.\n",
    "\n",
    "2. Execute the tool function with the generated `args`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tool name to function mapping\n",
    "name2tool = {tool.name: tool.func for tool in tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute to get our answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_exec_content = name2tool[tool_call.tool_calls[0][\"name\"]](\n",
    "    **tool_call.tool_calls[0][\"args\"]\n",
    ")\n",
    "tool_exec_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is our answer and tool execution logic. We feed this back into our LLM via the\n",
    "`agent_scratchpad` placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_t0dSdQbq2Strh6HfkzKirsX0', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 227, 'total_tokens': 245, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d3a4270f-0df9-4c13-89ff-99ed16b323d6-0', tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': 'call_t0dSdQbq2Strh6HfkzKirsX0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 227, 'output_tokens': 18, 'total_tokens': 245, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "tool_exec = ToolMessage(\n",
    "    content=f\"The {tool_call.tool_calls[0]['name']} tool returned {tool_exec_content}\",\n",
    "    tool_call_id=tool_call.tool_calls[0][\"id\"]\n",
    ")\n",
    "\n",
    "out = agent.invoke({\n",
    "    \"input\": \"What is 10 + 10\",\n",
    "    \"chat_history\": [],\n",
    "    \"agent_scratchpad\": [tool_call, tool_exec]\n",
    "})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite having the answer in our `agent_scratchpad`, the LLM still tries to use the tool\n",
    "_again_. This behaviour happens because we bonded the tools to the LLM with\n",
    "`tool_choice=\"any\"`. When we set `tool_choice` to `\"any\"` or `\"required\"`, we tell the\n",
    "LLM that it _MUST_ use a tool, i.e., it cannot provide a final answer.\n",
    "\n",
    "There's two options to fix this:\n",
    "\n",
    "1. Set `tool_choice=\"auto\"` to tell the LLM that it can choose to use a tool or provide\n",
    "a final answer.\n",
    "\n",
    "2. Create a `final_answer` tool - we'll explain this shortly.\n",
    "\n",
    "First, let's try option **1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"auto\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start from the start again, so `agent_scratchpad` is empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_3F1gvBL8dG7QP7R4B1hlW3yd', 'function': {'arguments': '{\"x\":10,\"y\":10}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 197, 'total_tokens': 215, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-bc6b9824-14a3-4537-85b6-9a51c0ff956d-0', tool_calls=[{'name': 'add', 'args': {'x': 10, 'y': 10}, 'id': 'call_3F1gvBL8dG7QP7R4B1hlW3yd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 197, 'output_tokens': 18, 'total_tokens': 215, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = agent.invoke({\"input\": \"What is 10 + 10\", \"chat_history\": []})\n",
    "tool_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we execute the tool and pass it's output into the `agent_scratchpad` placeholder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='10 + 10 equals 20.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 227, 'total_tokens': 237, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'stop', 'logprobs': None}, id='run-17c1bdf2-0596-4c88-9e17-a47abc47f4e5-0', usage_metadata={'input_tokens': 227, 'output_tokens': 10, 'total_tokens': 237, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_output = name2tool[tool_call.tool_calls[0][\"name\"]](\n",
    "    **tool_call.tool_calls[0][\"args\"]\n",
    ")\n",
    "\n",
    "tool_exec = ToolMessage(\n",
    "    content=f\"The {tool_call.tool_calls[0]['name']} tool returned {tool_output}\",\n",
    "    tool_call_id=tool_call.tool_calls[0][\"id\"]\n",
    ")\n",
    "\n",
    "out = agent.invoke({\n",
    "    \"input\": \"What is 10 + 10\",\n",
    "    \"chat_history\": [],\n",
    "    \"agent_scratchpad\": [tool_call, tool_exec]\n",
    "})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the final answer in the `content` field! This method is perfectly\n",
    "functional; however, we recommend option **2** as it provides more control over the\n",
    "agent's output.\n",
    "\n",
    "There are several reasons that option **2** can provide more control, those are:\n",
    "\n",
    "* It removes the possibility of an agent using the direct `content` field when it is not\n",
    "appropriate; for example, some LLMs (particularly smaller ones) may try to use the\n",
    "`content` field when using a tool.\n",
    "\n",
    "* We can enforce a specific structured output in our answers. Structured outputs are\n",
    "handy when we require particular fields for downstream code or multi-part answers. For\n",
    "example, a RAG agent may return a natural language answer and a list of sources used to\n",
    "generate that answer.\n",
    "\n",
    "To implement option **2**, we must create a `final_answer` tool. We will add a\n",
    "`tools_used` field to give our output some structure—in a real-world use case, we\n",
    "probably wouldn't want to generate this field, but it's useful for our example here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def final_answer(answer: str, tools_used: list[str]) -> str:\n",
    "    \"\"\"Use this tool to provide a final answer to the user.\n",
    "    The answer should be in natural language as this will be provided\n",
    "    to the user directly. The tools_used must include a list of tool\n",
    "    names that were used within the `scratchpad`.\n",
    "    \"\"\"\n",
    "    return {\"answer\": answer, \"tools_used\": tools_used}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `final_answer` tool _doesn't_ necessarily need to do anything; in this example,\n",
    "we're using it purely to structure our final response. We can now add this tool to our\n",
    "agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [final_answer, add, subtract, multiply, exponentiate]\n",
    "\n",
    "# we need to update our name2tool mapping too\n",
    "name2tool = {tool.name: tool.func for tool in tools}\n",
    "\n",
    "agent: RunnableSerializable = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we invoke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'x': 10, 'y': 10},\n",
       "  'id': 'call_xV2dNpFKGQj2wg9r0SK1keCO',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = agent.invoke({\"input\": \"What is 10 + 10\", \"chat_history\": []})\n",
    "tool_call.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the tool and provide it's output to the agent again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_tj2VLmz7oK9YdRZDCNwrAmNQ', 'function': {'arguments': '{\"answer\":\"10 + 10 equals 20.\",\"tools_used\":[\"functions.add\"]}', 'name': 'final_answer'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 299, 'total_tokens': 327, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e08ea162-646c-4dfa-ac15-980de6fc6052-0', tool_calls=[{'name': 'final_answer', 'args': {'answer': '10 + 10 equals 20.', 'tools_used': ['functions.add']}, 'id': 'call_tj2VLmz7oK9YdRZDCNwrAmNQ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 299, 'output_tokens': 28, 'total_tokens': 327, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_out = name2tool[tool_call.tool_calls[0][\"name\"]](\n",
    "    **tool_call.tool_calls[0][\"args\"]\n",
    ")\n",
    "\n",
    "tool_exec = ToolMessage(\n",
    "    content=f\"The {tool_call.tool_calls[0]['name']} tool returned {tool_out}\",\n",
    "    tool_call_id=tool_call.tool_calls[0][\"id\"]\n",
    ")\n",
    "\n",
    "out = agent.invoke({\n",
    "    \"input\": \"What is 10 + 10\",\n",
    "    \"chat_history\": [],\n",
    "    \"agent_scratchpad\": [tool_call, tool_exec]\n",
    "})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `content` remains empty because we force tool use. But we now have the\n",
    "`final_answer` tool, which the agent executor passes via the `tool_calls` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'final_answer',\n",
       "  'args': {'answer': '10 + 10 equals 20.', 'tools_used': ['functions.add']},\n",
       "  'id': 'call_tj2VLmz7oK9YdRZDCNwrAmNQ',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we see the `final_answer` tool here, we don't pass this back into our agent, and\n",
    "instead, this tells us to stop execution and pass the `args` output onto our downstream\n",
    "process or user directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '10 + 10 equals 20.', 'tools_used': ['functions.add']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.tool_calls[0][\"args\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Custom Agent Execution Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've worked through each step of our agent code, but it doesn't run without us running\n",
    "every step. We must write a class to handle all the logic we just worked through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "class CustomAgentExecutor:\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations: int = 3):\n",
    "        self.chat_history = []\n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"],\n",
    "                \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt\n",
    "            | llm.bind_tools(tools, tool_choice=\"any\")  # we're forcing tool use again\n",
    "        )\n",
    "\n",
    "    def invoke(self, input: str) -> dict:\n",
    "        # invoke the agent but we do this iteratively in a loop until\n",
    "        # reaching a final answer\n",
    "        count = 0\n",
    "        agent_scratchpad = []\n",
    "        while count < self.max_iterations:\n",
    "            # invoke a step for the agent to generate a tool call\n",
    "            tool_call = self.agent.invoke({\n",
    "                \"input\": input,\n",
    "                \"chat_history\": self.chat_history,\n",
    "                \"agent_scratchpad\": agent_scratchpad\n",
    "            })\n",
    "            # add initial tool call to scratchpad\n",
    "            agent_scratchpad.append(tool_call)\n",
    "            # otherwise we execute the tool and add it's output to the agent scratchpad\n",
    "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
    "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
    "            tool_call_id = tool_call.tool_calls[0][\"id\"]\n",
    "            tool_out = name2tool[tool_name](**tool_args)\n",
    "            # add the tool output to the agent scratchpad\n",
    "            tool_exec = ToolMessage(\n",
    "                content=f\"{tool_out}\",\n",
    "                tool_call_id=tool_call_id\n",
    "            )\n",
    "            agent_scratchpad.append(tool_exec)\n",
    "            # add a print so we can see intermediate steps\n",
    "            print(f\"{count}: {tool_name}({tool_args})\")\n",
    "            count += 1\n",
    "            # if the tool call is the final answer tool, we stop\n",
    "            if tool_name == \"final_answer\":\n",
    "                break\n",
    "        # add the final output to the chat history\n",
    "        final_answer = tool_out[\"answer\"]\n",
    "        self.chat_history.extend([\n",
    "            HumanMessage(content=input),\n",
    "            AIMessage(content=final_answer)\n",
    "        ])\n",
    "        # return the final answer in dict form\n",
    "        return json.dumps(tool_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialize the agent executor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = CustomAgentExecutor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test the `invoke` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: add({'x': 10, 'y': 10})\n",
      "1: final_answer({'answer': '10 + 10 equals 20.', 'tools_used': ['functions.add']})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"answer\": \"10 + 10 equals 20.\", \"tools_used\": [\"functions.add\"]}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(input=\"What is 10 + 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then get our answer and the tools that were used — all through our custom agent\n",
    "executor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
