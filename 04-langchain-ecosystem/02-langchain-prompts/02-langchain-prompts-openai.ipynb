{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts templates for OpenAI & Llama - LangChain #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most fundamental and underrated pieces of langchain is prompting, prompts are what we would send to our LLM to allow them to answer in a formulated response, super useful if we want to do something such as making NPCs speak with a specific accent or creating a song with a specifc twist in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'            \\nAnswer the question based on the context below,                 }\\nif you cannot answer the question using the                     }--->  (Rules) For Our Prompt\\nprovided information answer with \"I don\\'t know\"                 }\\n\\nContext: Aurelio AI is an AI development studio                 }\\nfocused on the fields of Natural Language Processing (NLP)      }\\nand information retrieval using modern tooling                  }--->   Context AI has\\nsuch as Large Language Models (LLMs),                           }\\nvector databases, and LangChain.                                }\\n\\nQuestion: Does Aurelio AI do anything related to LangChain?     }--->   User Question\\n\\nAnswer:                                                         }--->   AI Answer\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"            \n",
    "Answer the question based on the context below,                 }\n",
    "if you cannot answer the question using the                     }--->  (Rules) For Our Prompt\n",
    "provided information answer with \"I don't know\"                 }\n",
    "\n",
    "Context: Aurelio AI is an AI development studio                 }\n",
    "focused on the fields of Natural Language Processing (NLP)      }\n",
    "and information retrieval using modern tooling                  }--->   Context AI has\n",
    "such as Large Language Models (LLMs),                           }\n",
    "vector databases, and LangChain.                                }\n",
    "\n",
    "Question: Does Aurelio AI do anything related to LangChain?     }--->   User Question\n",
    "\n",
    "Answer:                                                         }--->   AI Answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is split into two versions - The [Ollama version], allowing us to run our LLM locally without needing any external services or API keys. The [OpenAI version] uses the OpenAI API and requires an OpenAI API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing OpenAI's gpt-4o-mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing the OpenAI model, fine-tuned for instruction following. We pull the model from OpenAI by switching to our terminal and executing:\n",
    "\n",
    "pip install openai\n",
    "\n",
    "Once the model libary has finished downloading, we initialize it in LangChain using the ChatOpenAI class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "openai_model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=openai_model, openai_api_key = OPENAI_API_KEY)\n",
    "creative_llm = ChatOpenAI(temperature=0.9, model=openai_model, openai_api_key = OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the AI will appoach our question, as you can see we have a formulated response, if the context has the answer, then use the context to answer the question, if not, say I don't know, then we also have context and question which are being passed into this similarly to paramaters in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt = \"\"\"\n",
    "Answer the question based on the context below,                 \n",
    "if you cannot answer the question using the                   \n",
    "provided information answer with \"I don't know\"\n",
    "\n",
    "context: {context}\n",
    "\n",
    "question: {query}\n",
    "\"\"\"\n",
    "Context = \"\"\"\n",
    "Context: Aurelio AI is an AI development studio                 \n",
    "focused on the fields of Natural Language Processing (NLP)      \n",
    "and information retrieval using modern tooling                  \n",
    "such as Large Language Models (LLMs),                           \n",
    "vector databases, and LangChain. Owned by James Briggs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we want to create our template we will use for our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Passing the template to the LangChain model.\n",
    "prompt_template = ChatPromptTemplate.from_template(Prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply any question into here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = \"Who owns Aurelio AI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we're converting the message into the correct format for our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt_template.format_messages(prompt=Prompt, query=Question, context=Context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will retrieve our answer and showcase the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James Briggs owns Aurelio AI.\n"
     ]
    }
   ],
   "source": [
    "answer = llm(messages)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works for the basic chatbots, however what if we want to give some questions and answers so that the AI can have a rough idea of what we want it to say?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Few Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will cover Few Shot Prompt Templates allowing us to be able to go more in depth with our AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Firstly we want to add a example template, this will tell the AI how we would like to format our inputs and outputs, in this case, the questions and answers.\n",
    "2. Secondly we want to add a prompt template, this again will be used to tell the AI how we would like it to use the examples and answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the example template\n",
    "example_template = \"\"\"Input: {input}\n",
    "Output: {output}\"\"\"\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"Given the following examples, complete the task:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of questions and answers, here you can see that we're trying to match words based on their similarity to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples = [\n",
    "        {\"input\": \"Clouds, Sky, Sun, Space, Planets\", \"output\": \"Galaxy\"},\n",
    "        {\"input\": \"Eyes, Face, Human, Animal\", \"output\": \"Species\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to make a few shot template where we can feed all this data into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=Examples,\n",
    "    example_prompt=PromptTemplate.from_template(example_template),\n",
    "    prefix = prompt_template,\n",
    "    suffix=\"Input: {input}\\nOutput:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can carry on with our question to see what it thinks is similar too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following examples, complete the task:\n",
      "\n",
      "Input: Clouds, Sky, Sun, Space, Planets\n",
      "Output: Galaxy\n",
      "\n",
      "Input: Eyes, Face, Human, Animal\n",
      "Output: Species\n",
      "\n",
      "Input: Fitness, Health, Lifestyle\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt with input\n",
    "formatted_prompt = few_shot_prompt.format(input=\"Fitness, Health, Lifestyle\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to run this through our model and see what the output is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wellness\n"
     ]
    }
   ],
   "source": [
    "output = creative_llm(messages=[HumanMessage(content=formatted_prompt)])\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However that is considered the old way of doing few shot prompts, here below we can see a newer way which is way easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Clouds, Sky, Sun, Space, Planets\n",
      "AI: Galaxy\n",
      "Human: Eyes, Face, Human, Animal\n",
      "AI: Species\n"
     ]
    }
   ],
   "source": [
    "# This is a prompt template used to format each individual example.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "# This is our few shot template used to feed our examples into the LLM\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=Examples,\n",
    ")\n",
    "# Here we can view the format of our content down below\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a poem writer.'), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': 'Clouds, Sky, Sun, Space, Planets', 'output': 'Galaxy'}, {'input': 'Eyes, Face, Human, Animal', 'output': 'Species'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Here we are formatting our final prompt for the AI to use, take note that the input needs to be the same keyword as the input below.\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a poem writer.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "print(final_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Fitness, Health, Lifestyle', 'text': 'Wellness'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=final_prompt, llm=creative_llm)\n",
    "\n",
    "chain.invoke({\"input\": \"Fitness, Health, Lifestyle\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to dive into Chain-Of-Thought prompting, for this there is no direct prompt template however instead what we do is we set the template up to make the AI talk to us about how it will solve the problem, by going through each step rather then just rushing straight to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will settup the step by step rule to enable chain of thought prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "First, list systematically and in detail all the problems in this question\n",
    "that need to be solved before we can arrive at the correct answer.\n",
    "Then, solve each sub problem using the answers of previous problems\n",
    "and reach a final solution\n",
    "\n",
    "Output: Well the right answer is... \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "template2 = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Do not use any chain-of-thought processes to answer the question\n",
    "\n",
    "Output: Well the wrong answer is... \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the rest is the same as a simple prompt, where we feed a basic question into the LLM and it will give us a answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "prompt2 = PromptTemplate(template=template2, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the question-answering chain with the chain-of-thought prompt\n",
    "chain = LLMChain(prompt = prompt, llm = llm)\n",
    "# Load the question-answering chain with the chain-of-thought prompt\n",
    "chain2 = LLMChain(prompt = prompt2, llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "question = \"How many keystrokes are needed to type the numbers from 1 to 500? Answer Choices:\" # 1392 is the correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how many keystrokes are needed to type the numbers from 1 to 500, we need to break down the problem into several sub-problems. Here’s a systematic approach to solving it:\n",
      "\n",
      "### Sub-Problem 1: Count the Number of Digits in Each Range\n",
      "\n",
      "1. **Count the numbers from 1 to 9**:\n",
      "   - There are 9 numbers (1, 2, 3, 4, 5, 6, 7, 8, 9).\n",
      "   - Each number has 1 digit.\n",
      "   - Total keystrokes for this range = 9 numbers × 1 digit = 9 keystrokes.\n",
      "\n",
      "2. **Count the numbers from 10 to 99**:\n",
      "   - There are 90 numbers (10 to 99).\n",
      "   - Each number has 2 digits.\n",
      "   - Total keystrokes for this range = 90 numbers × 2 digits = 180 keystrokes.\n",
      "\n",
      "3. **Count the numbers from 100 to 499**:\n",
      "   - There are 400 numbers (100 to 499).\n",
      "   - Each number has 3 digits.\n",
      "   - Total keystrokes for this range = 400 numbers × 3 digits = 1200 keystrokes.\n",
      "\n",
      "4. **Count the number 500**:\n",
      "   - There is 1 number (500).\n",
      "   - It has 3 digits.\n",
      "   - Total keystrokes for this number = 1 number × 3 digits = 3 keystrokes.\n",
      "\n",
      "### Sub-Problem 2: Sum All Keystrokes\n",
      "\n",
      "Now, we need to sum the total keystrokes from each of the ranges we calculated:\n",
      "\n",
      "- Keystrokes from 1 to 9: 9\n",
      "- Keystrokes from 10 to 99: 180\n",
      "- Keystrokes from 100 to 499: 1200\n",
      "- Keystrokes for 500: 3\n",
      "\n",
      "### Final Calculation\n",
      "\n",
      "Total keystrokes = 9 + 180 + 1200 + 3 = 1392\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Well, the right answer is **1392 keystrokes** needed to type the numbers from 1 to 500.\n"
     ]
    }
   ],
   "source": [
    "result = chain.run(question)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well the wrong answer is 1500.\n"
     ]
    }
   ],
   "source": [
    "result = chain2.run(question)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see with the amount of steps involved without using a step by step guide, the AI can step over important information that would help it achieve the correct results, however it's important to note that without a super long question the AI can solve alot of these issues without using chain prompting, however it's just in these certain instances where chain prompting can be very useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
