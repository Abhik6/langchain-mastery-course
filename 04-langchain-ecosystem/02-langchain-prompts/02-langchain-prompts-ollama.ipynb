{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts templates for OpenAI & Llama - LangChain #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most fundamental and underrated pieces of langchain is prompting, prompts are what we would send to our LLM to allow them to answer in a formulated response, super useful if we want to do something such as making NPCs speak with a specific accent or creating a song with a specifc twist in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'            \\nAnswer the question based on the context below,                 }\\nif you cannot answer the question using the                     }--->  (Rules) For Our Prompt\\nprovided information answer with \"I don\\'t know\"                 }\\n\\nContext: Aurelio AI is an AI development studio                 }\\nfocused on the fields of Natural Language Processing (NLP)      }\\nand information retrieval using modern tooling                  }--->   Context AI has\\nsuch as Large Language Models (LLMs),                           }\\nvector databases, and LangChain.                                }\\n\\nQuestion: Does Aurelio AI do anything related to LangChain?     }--->   User Question\\n\\nAnswer:                                                         }--->   AI Answer\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"            \n",
    "Answer the question based on the context below,                 }\n",
    "if you cannot answer the question using the                     }--->  (Rules) For Our Prompt\n",
    "provided information answer with \"I don't know\"                 }\n",
    "\n",
    "Context: Aurelio AI is an AI development studio                 }\n",
    "focused on the fields of Natural Language Processing (NLP)      }\n",
    "and information retrieval using modern tooling                  }--->   Context AI has\n",
    "such as Large Language Models (LLMs),                           }\n",
    "vector databases, and LangChain.                                }\n",
    "\n",
    "Question: Does Aurelio AI do anything related to LangChain?     }--->   User Question\n",
    "\n",
    "Answer:                                                         }--->   AI Answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is split into two versions - The [Ollama version], allowing us to run our LLM locally without needing any external services or API keys. The [OpenAI version] uses the OpenAI API and requires an OpenAI API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Llama 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing the 1B parameter Llama 3.2 model, fine-tuned for instruction following. We pull the model from Ollama by switching to our terminal and executing:\n",
    "\n",
    "ollama pull llama3.2:1b-instruct-fp16\n",
    "\n",
    "Once the model has finished downloading, we initialize it in LangChain using the ChatOllama class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model_name = \"llama3.2:1b-instruct-fp16\"\n",
    "\n",
    "# initialize one LLM with temperature 0.0, this makes the LLM more deterministic\n",
    "llm = ChatOllama(temperature=0.0, model=model_name)\n",
    "\n",
    "# initialize another LLM with temperature 0.9, this makes the LLM more creative\n",
    "creative_llm = ChatOllama(temperature=0.9, model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the AI will appoach our question, as you can see we have a formulated response, if the context has the answer, then use the context to answer the question, if not, say I don't know, then we also have context and question which are being passed into this similarly to paramaters in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt = \"\"\"\n",
    "Answer the question based on the context below,                 \n",
    "if you cannot answer the question using the                   \n",
    "provided information answer with \"I don't know\"\n",
    "\n",
    "context: {context}\n",
    "\n",
    "question: {query}\n",
    "\"\"\"\n",
    "Context = \"\"\"\n",
    "Context: Aurelio AI is an AI development studio                 \n",
    "focused on the fields of Natural Language Processing (NLP)      \n",
    "and information retrieval using modern tooling                  \n",
    "such as Large Language Models (LLMs),                           \n",
    "vector databases, and LangChain. Owned by James Briggs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we want to create our template we will use for our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Passing the template to the LangChain model.\n",
    "prompt_template = ChatPromptTemplate.from_template(Prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply any question into here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = \"Who owns Aurelio AI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we're converting the message into the correct format for our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt_template.format_messages(prompt=Prompt, query=Question, context=Context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will retrieve our answer and showcase the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joshu\\AppData\\Local\\Temp\\ipykernel_53560\\1989704094.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = llm(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "answer = llm(messages)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works for the basic chatbots, however what if we want to give some questions and answers so that the AI can have a rough idea of what we want it to say?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Few Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will cover Few Shot Prompt Templates allowing us to be able to go more in depth with our AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Firstly we want to add a example template, this will tell the AI how we would like to format our inputs and outputs, in this case, the questions and answers.\n",
    "2. Secondly we want to add a prompt template, this again will be used to tell the AI how we would like it to use the examples and answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the example template\n",
    "example_template = \"\"\"Input: {input}\n",
    "Output: {output}\"\"\"\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"Given the following examples, complete the task:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of questions and answers, here you can see that we're trying to match words based on their similarity to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples = [\n",
    "        {\"input\": \"Clouds, Sky, Sun, Space, Planets\", \"output\": \"Galaxy\"},\n",
    "        {\"input\": \"Eyes, Face, Human, Animal\", \"output\": \"Species\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to make a few shot template where we can feed all this data into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=Examples,\n",
    "    example_prompt=PromptTemplate.from_template(example_template),\n",
    "    prefix = prompt_template,\n",
    "    suffix=\"Input: {input}\\nOutput:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can carry on with our question to see what it thinks is similar too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following examples, complete the task:\n",
      "\n",
      "Input: Clouds, Sky, Sun, Space, Planets\n",
      "Output: Galaxy\n",
      "\n",
      "Input: Eyes, Face, Human, Animal\n",
      "Output: Species\n",
      "\n",
      "Input: Fitness, Health, Lifestyle\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt with input\n",
    "formatted_prompt = few_shot_prompt.format(input=\"Fitness, Health, Lifestyle\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to run this through our model and see what the output is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help you complete the tasks.\n",
      "\n",
      "Here are the completed examples:\n",
      "\n",
      " Input: Clouds, Sky, Sun, Space, Planets\n",
      " Output: Galaxy\n",
      "\n",
      " Input: Eyes, Face, Human, Animal\n",
      " Output: Kingdom\n",
      "\n",
      " Input: Fitness, Health, Lifestyle\n",
      " Output: Species\n"
     ]
    }
   ],
   "source": [
    "output = creative_llm(messages=[HumanMessage(content=formatted_prompt)])\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However that is considered the old way of doing few shot prompts, here below we can see a newer way which is way easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Clouds, Sky, Sun, Space, Planets\n",
      "AI: Galaxy\n",
      "Human: Eyes, Face, Human, Animal\n",
      "AI: Species\n"
     ]
    }
   ],
   "source": [
    "# This is a prompt template used to format each individual example.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "# This is our few shot template used to feed our examples into the LLM\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=Examples,\n",
    ")\n",
    "# Here we can view the format of our content down below\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a poem writer.'), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': 'Clouds, Sky, Sun, Space, Planets', 'output': 'Galaxy'}, {'input': 'Eyes, Face, Human, Animal', 'output': 'Species'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Here we are formatting our final prompt for the AI to use, take note that the input needs to be the same keyword as the input below.\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a poem writer.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "print(final_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joshu\\AppData\\Local\\Temp\\ipykernel_53560\\3236369251.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=final_prompt, llm=creative_llm)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Fitness, Health, Lifestyle', 'text': 'Wellness'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=final_prompt, llm=creative_llm)\n",
    "\n",
    "chain.invoke({\"input\": \"Fitness, Health, Lifestyle\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to dive into Chain-Of-Thought prompting, for this there is no direct prompt template however instead what we do is we set the template up to make the AI talk to us about how it will solve the problem, by going through each step rather then just rushing straight to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will settup the step by step rule to enable chain of thought prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "First, list systematically and in detail all the problems in this question\n",
    "that need to be solved before we can arrive at the correct answer.\n",
    "Then, solve each sub problem using the answers of previous problems\n",
    "and reach a final solution\n",
    "\n",
    "Output: Well the right answer is... \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "template2 = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Do not use any chain-of-thought processes to answer the question\n",
    "\n",
    "Output: Well the wrong answer is... \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the rest is the same as a simple prompt, where we feed a basic question into the LLM and it will give us a answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "prompt2 = PromptTemplate(template=template2, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the question-answering chain with the chain-of-thought prompt\n",
    "chain = LLMChain(prompt = prompt, llm = llm)\n",
    "# Load the question-answering chain with the chain-of-thought prompt\n",
    "chain2 = LLMChain(prompt = prompt2, llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "question = \"James has 7 apples, he eats 4 and is given an additional 19 apples, James gives 15 apples to Josh, and Josh gives James 2 apples, how many apples does James have?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joshu\\AppData\\Local\\Temp\\ipykernel_53560\\2242271788.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the problems that need to be solved systematically and in detail:\n",
      "\n",
      "1. James has 7 apples initially.\n",
      "2. James eats 4 apples.\n",
      "3. James receives an additional 19 apples.\n",
      "4. James gives 15 apples to Josh.\n",
      "5. James gives 2 apples to Josh.\n",
      "\n",
      "To solve this problem, we will follow these steps:\n",
      "\n",
      "**Step 1: Calculate the number of apples James has after eating 4**\n",
      "\n",
      "James starts with 7 apples and eats 4, so he is left with:\n",
      "7 - 4 = 3\n",
      "\n",
      "**Step 2: Add the additional 19 apples that James receives**\n",
      "\n",
      "Now, James has 3 apples and receives an additional 19 apples. So, we add these two numbers together:\n",
      "3 + 19 = 22\n",
      "\n",
      "**Step 3: Subtract the number of apples James gives to Josh**\n",
      "\n",
      "James gives 15 apples to Josh, so he is left with:\n",
      "22 - 15 = 7\n",
      "\n",
      "**Step 4: Add the number of apples James gives to Josh**\n",
      "\n",
      "James gives 2 apples to Josh, so we add these two numbers together:\n",
      "7 + 2 = 9\n",
      "\n",
      "Therefore, the correct answer is: James has 9 apples.\n"
     ]
    }
   ],
   "source": [
    "result = chain.run(question)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 - 4 + 19 + 0 (Josh's apples) + 15 + 2 = 23\n"
     ]
    }
   ],
   "source": [
    "result = chain2.run(question)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see with the amount of steps involved without using a step by step guide, the AI can step over important information that would help it achieve the correct results, however it's important to note that without a super long question the AI can solve alot of these issues without using chain prompting, however it's just in these certain instances where chain prompting can be very useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
