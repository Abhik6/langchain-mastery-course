{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts Templating for Ollama - LangChain #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until 2021, to use an AI model for a specific use-case we would need to fine-tune the model weights themselves. That would require huge amounts of training data and significant compute to fine-tune any reasonably performing model.\n",
    "\n",
    "Instruction fine-tuned **L**arge **L**anguage **M**odels (LLMs) changed this fundamental rule of applying AI models to new use-cases. Rather than needing to either train a model from scratch or fine-tune an existing model, these new LLMs could adapt incredibly well to a new problem or use-case with nothing more than a prompt change.\n",
    "\n",
    "Prompts allow us to completely change the functionality of an AI pipeline. Through natural language we simply _tell_ our LLM what it needs to do, and with the right AI pipeline and prompting, it often works.\n",
    "\n",
    "LangChain naturally has many functionalities geared towards helping us build our prompts. We can build very dynamic prompting pipelines that modifying the structure and content of what we feed into our LLM based on essentially any parameter we would like. In this example, we'll explore the essentials to prompting in LangChain and apply this in a demo **R**etrieval **A**ugmented **G**eneration (RAG) pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> !!! We will be using Ollama for this example allowing us to run everything locally. If you would like to use OpenAI instead, please see the [OpenAI version] TK of this example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by looking at the various parts of our prompt. For RAG use-cases we'll typically have three core components however this is _very_ use-cases dependant and can vary significantly. Nonetheless, for RAG we will typically see:\n",
    "\n",
    "* **Rules for our LLM**: this part of the prompt sets up the behavior of our LLM, how it should approach responding to user queries, and simply providing as much information as possible about what we're wanting to do as possible. We typically place this within the _system prompt_ of an chat LLM.\n",
    "\n",
    "* **Context**: this part is RAG-specific. The context refers to some _external information_ that we may have retrieved from a web search, database query, or often a _vector database_. This external information is the **R**etrieval **A**ugmentation part of **RA**G. For chat LLMs we'll typically place this inside the chat messages between the assistant and user.\n",
    "\n",
    "* **Question**: this is the input from our user. In the vast majority of cases the question/query/user input will always be provided to the LLM (and typically through a _user message_). However, the format and location of this being provided often changes.\n",
    "\n",
    "* **Answer**: this is the answer from our assistant, again this is _very_ typical and we'd expect this with every use-case.\n",
    "\n",
    "The below is an example of how a RAG prompt may look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Answer the question based on the context below,                 }\n",
    "if you cannot answer the question using the                     }--->  (Rules) For Our Prompt\n",
    "provided information answer with \"I don't know\"                 }\n",
    "\n",
    "Context: Aurelio AI is an AI development studio                 }\n",
    "focused on the fields of Natural Language Processing (NLP)      }\n",
    "and information retrieval using modern tooling                  }--->   Context AI has\n",
    "such as Large Language Models (LLMs),                           }\n",
    "vector databases, and LangChain.                                }\n",
    "\n",
    "Question: Does Aurelio AI do anything related to LangChain?     }--->   User Question\n",
    "\n",
    "Answer:                                                         }--->   AI Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the AI will appoach our question, as you can see we have a formulated response, if the context has the answer, then use the context to answer the question, if not, say I don't know, then we also have context and question which are being passed into this similarly to paramaters in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the user's query based on the context below.                 \n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain uses a `ChatPromptTemplate` object to format the various prompt types into a single list which will be passed to our LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# passing the template to the LangChain model\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt),\n",
    "    (\"user\", \"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call the template it will expect us to provide two variables, the `context` and the `query`. Both of these variables are pulled from the strings we wrote, as LangChain interprets curly-bracket syntax (ie `{context}` and `{query}`) as indicating a dynamic variable that we expect to be inserted at query time. We can see that these variables have been picked up by our template object by viewing it's `input_variables` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'query']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the structure of the messages (currently _prompt templates_) that the `ChatPromptTemplate` will construct by viewing the `messages` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context below,                 \\nif you cannot answer the question using the                   \\nprovided information answer with \"I don\\'t know\"\\n\\ncontext: {context}\\n\\nquestion: {query}\\n'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see that each tuple provided when using `ChatPromptTemplate.from_messages` becomes an individual prompt template itself. Within each of these tuples, the first value defines the _role_ of the message, which is typically `system`, `human`, or `ai`. Using these tuples is shorthand for the following, more explicit code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the structure of this new chat prompt template is identical to our previous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context below,                 \\nif you cannot answer the question using the                   \\nprovided information answer with \"I don\\'t know\"\\n\\ncontext: {context}\\n\\nquestion: {query}\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoking our LLM with Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined our prompt template, now let's define out LLM and run it with our template and a user query.\n",
    "\n",
    "First, we initialize our LLM. For this, we are using Ollama\n",
    "\n",
    "We start by initializing the 1B parameter Llama 3.2 model, fine-tuned for instruction following. We pull the model from Ollama by switching to our terminal and executing:\n",
    "\n",
    "```\n",
    "ollama pull llama3.2:1b-instruct-fp16\n",
    "```\n",
    "\n",
    "Once the model has finished downloading, we initialize it in LangChain using the ChatOllama class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model_name = \"llama3.2:1b-instruct-fp16\"\n",
    "\n",
    "# initialize one LLM with temperature 0.0, this makes the LLM more deterministic\n",
    "llm = ChatOllama(temperature=0.0, model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our LLM and _because_ we're using it for a question-answer use-case we want it's answer to be as grounded in reality as possible. To do that, we ofcourse prompt it to not make up any information via the `If you cannot answer the question using the provided information answer with \"I don't know\"` line, but we _also_ use the model's `temperature` setting.\n",
    "\n",
    "The `temperature` parameter controls the randomness of the LLM's output. A temperature of `0.0` makes an LLM's output more determinstic which _in theory_ should lead to a lower likelihood of hallucination.\n",
    "\n",
    "Now, the question here may be, _why would we ever not use `temperature=0.0`?_ The answer to that is that sometimes a little bit of randomness can useful. Randomness tends to translate to text that feels more human and creative, so if we'd like an LLM to help us write an article or even a poem, that lack of determinism becomes a feature rather than a bug.\n",
    "\n",
    "For now, we'll stick with our more deterministic LLM. We'll setup the pipeline to consume two variables when our LLM pipeline is called, `query` and `context`, we'll feed them into our chat prompt template, and then invoke our LLM with our formatted messages.\n",
    "\n",
    "Although that sounds complicated, all we're doing is connecting our `prompt_template` and `llm`. We do this with **L**ang**C**hain **E**xpression **L**anguage (LCEL), which uses the `|` operator to connect our each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a `query` and some relevant `context` and invoke our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Aurelio AI is an AI company developing tooling for AI\n",
    "engineers. Their focus is on language AI with the team having strong\n",
    "expertise in building AI agents and a strong background in\n",
    "information retrieval.\n",
    "\n",
    "The company is behind several open source frameworks, most notably\n",
    "Semantic Router and Semantic Chunkers. They also have an AI\n",
    "Platform providing engineers with tooling to help them build with\n",
    "AI. Finally, the team also provides development services to other\n",
    "organizations to help them bring their AI tech to market.\n",
    "\n",
    "Aurelio AI became LangChain Partners in September 2024 after a long\n",
    "track record of delivering AI solutions built with the LangChain\n",
    "ecosystem.\"\"\"\n",
    "\n",
    "query = \"what does Aurelio AI do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='According to the context, Aurelio AI is an AI company that develops tooling for AI engineers. They focus on language AI and have expertise in building AI agents and information retrieval. Additionally, they provide several tools and services, including:\\n\\n* Open source frameworks: Semantic Router and Semantic Chunkers\\n* AI Platform: providing engineers with tooling to build with AI\\n* Development services: helping other organizations bring their AI technology to market.\\n\\nAurelio AI became LangChain Partners in September 2024 after a long track record of delivering AI solutions built with the LangChain ecosystem.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b-instruct-fp16', 'created_at': '2024-12-04T18:00:32.646486Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1003948000, 'load_duration': 26077708, 'prompt_eval_count': 209, 'prompt_eval_duration': 95000000, 'eval_count': 117, 'eval_duration': 881000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-5ff1ce56-12b5-45cf-9500-de4224aa75ea-0', usage_metadata={'input_tokens': 209, 'output_tokens': 117, 'total_tokens': 326})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.invoke({\"query\": query, \"context\": context})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LLM pipeline is able to consume the information from the `context` and use it to answer the user's `query`. Ofcourse, we would never realistically be feeding in both a question and an answer into an LLM manually. Typically, the `context` would be retrieved from a vector database, via web search, or from elsewhere. We will cover this use-case in full and build a functional RAG pipeline in a future chapter.\n",
    "\n",
    "For now, we'll continue with the essentials of prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However that is considered the old way of doing few shot prompts, here below we can see a newer way which is way easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many **S**tate-**o**f-**t**he-**A**rt (SotA) LLMs are incredible at instruction following. Meaning that it requires much less effort to get the intended output or behavior from these models than is the case for older LLMs and smaller LLMs.\n",
    "\n",
    "We're using a _one billion_ parameter LLM, that's where the `:1b-` part of `llama3.2:1b-instruct-fp16` comes from. Now, that may seem like a lot but in the world of LLMs this is a _tiny_ model. Because of it's size it can be efficiently and even used easily on a lot of consumer hardware. However, it is also less capable than other models like `gpt-4o` or `claude-3.5-sonnet`.\n",
    "\n",
    "Using our tiny LLM does mean we need to put in a little extra work to get to generate what we'd like it to generate. Let's try an example, we'll ask the LLM to summarize the key points about Aurelio AI using markdown and bullet points. Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# What Does Aurelio AI Do?\n",
      "\n",
      "Aurelio AI is an AI company that develops tooling for AI engineers, focusing on language AI. They have a strong background in building AI agents and expertise in information retrieval.\n",
      "\n",
      "## Key Areas of Work\n",
      "\n",
      "*   Developing open-source frameworks:\n",
      "    *   Semantic Router\n",
      "    *   Semantic Chunkers\n",
      "*   Providing AI Platform:\n",
      "    *   Tooling for building AI with the LangChain ecosystem\n",
      "*   Offering Development Services:\n",
      "\n",
      "    *   To other organizations to help them bring their AI technology to market\n"
     ]
    }
   ],
   "source": [
    "new_system_prompt = \"\"\"\n",
    "Answer the user's query based on the context below.                 \n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Always answer in markdown format. When doing so please\n",
    "provide headers, short summaries, follow with bullet\n",
    "points, then conclude.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template.messages[0].prompt.template = new_system_prompt\n",
    "\n",
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display our markdown nicely with `IPython` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# What Does Aurelio AI Do?\n",
       "\n",
       "Aurelio AI is an AI company that develops tooling for AI engineers, focusing on language AI. They have a strong background in building AI agents and expertise in information retrieval.\n",
       "\n",
       "## Key Areas of Work\n",
       "\n",
       "*   Developing open-source frameworks:\n",
       "    *   Semantic Router\n",
       "    *   Semantic Chunkers\n",
       "*   Providing AI Platform:\n",
       "    *   Tooling for building AI with the LangChain ecosystem\n",
       "*   Offering Development Services:\n",
       "\n",
       "    *   To other organizations to help them bring their AI technology to market"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not bad, but also not quite the format we wanted. We can try fine-tuning and tweaking our prompt instructions further, or we can also provide some examples of what we'd like. Providing examples is what we'd refer to as _few-shot prompting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_system_prompt = \"\"\"\n",
    "Answer the user's query based on the context below.                 \n",
    "If you cannot answer the question using the\n",
    "provided information answer with \"I don't know\".\n",
    "\n",
    "Always answer in markdown format. When doing so please\n",
    "provide headers, short summaries, follow with bullet\n",
    "points, then conclude. Here are some examples:\n",
    "\n",
    "\n",
    "User: Can you explain gravity?\n",
    "AI: ## Gravity\n",
    "\n",
    "Gravity is one of the fundamental forces in the universe.\n",
    "\n",
    "### Discovery\n",
    "\n",
    "* Gravity was first discovered by Sir Isaac Newton in the late 17th century.\n",
    "* It was said that Newton theorized about gravity after seeing an apple fall from a tree.\n",
    "\n",
    "### In General Relativity\n",
    "\n",
    "* Gravity is described as the curvature of spacetime.\n",
    "* The more massive an object is, the more it curves spacetime.\n",
    "* This curvature is what causes objects to fall towards each other.\n",
    "\n",
    "### Gravitons\n",
    "\n",
    "* Gravitons are hypothetical particles that mediate the force of gravity.\n",
    "* They have not yet been detected.\n",
    "\n",
    "**To conclude**, Gravity is a fascinating topic and has been studied extensively since the time of Newton.\n",
    "\n",
    "\n",
    "User: What is the capital of France?\n",
    "AI: ## France\n",
    "\n",
    "The capital of France is Paris.\n",
    "\n",
    "### Origins\n",
    "\n",
    "* The name Paris comes from the Latin word \"Parisini\" which referred to a Celtic people living in the area.\n",
    "* The Romans named the city Lutetia, which means \"the place where the river turns\".\n",
    "* The city was renamed Paris in the 3rd century BC by the Celtic-speaking Parisii tribe.\n",
    "\n",
    "### Famous Landmarks\n",
    "\n",
    "* The Eiffel Tower\n",
    "* The Louvre\n",
    "* Notre-Dame Cathedral\n",
    "\n",
    "**To conclude**, Paris is highly regarded as one of the most beautiful cities in the world and is one of the world's\n",
    "greatest cultural and economic centres.\n",
    "\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template.messages[0].prompt.template = new_system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Aurelio AI Overview**\n",
       "\n",
       "Aurelio AI is an AI company that develops tooling for AI engineers, focusing on language AI and information retrieval.\n",
       "\n",
       "### Key Areas of Expertise\n",
       "\n",
       "* **Language AI**: Aurelio AI has strong expertise in building AI agents and provides tools to help engineers build with AI.\n",
       "* **Information Retrieval**: The company also specializes in information retrieval, providing frameworks and platforms to aid in data analysis and search.\n",
       "* **AI Platform**: Aurelio AI offers an AI platform that enables engineers to develop and deploy AI solutions.\n",
       "\n",
       "### Tooling and Frameworks\n",
       "\n",
       "Aurelio AI's tooling includes:\n",
       "\n",
       "* **Semantic Router**: A framework for building semantic search systems\n",
       "* **Semantic Chunkers**: A set of tools for chunking and disambiguating text data\n",
       "* **LangChain Platform**: An AI platform providing a range of tools and services for language AI development\n",
       "\n",
       "### Services\n",
       "\n",
       "Aurelio AI provides development services to organizations, helping them bring their AI technology to market.\n",
       "\n",
       "**To conclude**, Aurelio AI is a company that specializes in developing tooling and platforms for AI engineers, with a focus on language AI and information retrieval."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = pipeline.invoke({\"query\": query, \"context\": context}).content\n",
    "\n",
    "display(Markdown(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by adding a few examples to our prompt, ie _few-shot prompting_, we can get much more control over the exact structure of our LLM response. As the size of our LLMs increases, the ability of them to follow instructions becomes much greater and they tend to require less explicit prompting as we have shown here. However, even for SotA models like `gpt-4o` few-shot prompting is still a valid technique that can be used if the LLM is struggling to follow our intended instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Clouds, Sky, Sun, Space, Planets\n",
      "AI: Galaxy\n",
      "Human: Eyes, Face, Human, Animal\n",
      "AI: Species\n"
     ]
    }
   ],
   "source": [
    "# This is a prompt template used to format each individual example.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "# This is our few shot template used to feed our examples into the LLM\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=Examples,\n",
    ")\n",
    "# Here we can view the format of our content down below\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a poem writer.'), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': 'Clouds, Sky, Sun, Space, Planets', 'output': 'Galaxy'}, {'input': 'Eyes, Face, Human, Animal', 'output': 'Species'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Here we are formatting our final prompt for the AI to use, take note that the input needs to be the same keyword as the input below.\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a poem writer.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "print(final_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joshu\\AppData\\Local\\Temp\\ipykernel_53560\\3236369251.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=final_prompt, llm=creative_llm)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Fitness, Health, Lifestyle', 'text': 'Wellness'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=final_prompt, llm=creative_llm)\n",
    "\n",
    "chain.invoke({\"input\": \"Fitness, Health, Lifestyle\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to dive into Chain-Of-Thought prompting, for this there is no direct prompt template however instead what we do is we set the template up to make the AI talk to us about how it will solve the problem, by going through each step rather then just rushing straight to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will settup the step by step rule to enable chain of thought prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "First, list systematically and in detail all the problems in this question\n",
    "that need to be solved before we can arrive at the correct answer.\n",
    "Then, solve each sub problem using the answers of previous problems\n",
    "and reach a final solution\n",
    "\n",
    "Output: Well the right answer is... \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "template2 = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Do not use any chain-of-thought processes to answer the question\n",
    "\n",
    "Output: Well the wrong answer is... \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the rest is the same as a simple prompt, where we feed a basic question into the LLM and it will give us a answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "prompt2 = PromptTemplate(template=template2, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the question-answering chain with the chain-of-thought prompt\n",
    "chain = LLMChain(prompt = prompt, llm = llm)\n",
    "# Load the question-answering chain with the chain-of-thought prompt\n",
    "chain2 = LLMChain(prompt = prompt2, llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "question = \"James has 7 apples, he eats 4 and is given an additional 19 apples, James gives 15 apples to Josh, and Josh gives James 2 apples, how many apples does James have?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joshu\\AppData\\Local\\Temp\\ipykernel_53560\\2242271788.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the problems that need to be solved systematically and in detail:\n",
      "\n",
      "1. James has 7 apples initially.\n",
      "2. James eats 4 apples.\n",
      "3. James receives an additional 19 apples.\n",
      "4. James gives 15 apples to Josh.\n",
      "5. James gives 2 apples to Josh.\n",
      "\n",
      "To solve this problem, we will follow these steps:\n",
      "\n",
      "**Step 1: Calculate the number of apples James has after eating 4**\n",
      "\n",
      "James starts with 7 apples and eats 4, so he is left with:\n",
      "7 - 4 = 3\n",
      "\n",
      "**Step 2: Add the additional 19 apples that James receives**\n",
      "\n",
      "Now, James has 3 apples and receives an additional 19 apples. So, we add these two numbers together:\n",
      "3 + 19 = 22\n",
      "\n",
      "**Step 3: Subtract the number of apples James gives to Josh**\n",
      "\n",
      "James gives 15 apples to Josh, so he is left with:\n",
      "22 - 15 = 7\n",
      "\n",
      "**Step 4: Add the number of apples James gives to Josh**\n",
      "\n",
      "James gives 2 apples to Josh, so we add these two numbers together:\n",
      "7 + 2 = 9\n",
      "\n",
      "Therefore, the correct answer is: James has 9 apples.\n"
     ]
    }
   ],
   "source": [
    "result = chain.run(question)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 - 4 + 19 + 0 (Josh's apples) + 15 + 2 = 23\n"
     ]
    }
   ],
   "source": [
    "result = chain2.run(question)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see with the amount of steps involved without using a step by step guide, the AI can step over important information that would help it achieve the correct results, however it's important to note that without a super long question the AI can solve alot of these issues without using chain prompting, however it's just in these certain instances where chain prompting can be very useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
