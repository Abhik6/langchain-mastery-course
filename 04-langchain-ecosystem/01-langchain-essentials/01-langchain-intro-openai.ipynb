{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain Essentials Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
    "\n",
    "In this example, we will introduce LangChain, building a simple LLM-powered assistant. We'll provide examples for both OpenAI's `gpt-4o-mini` *and* Meta's `llama3.2` via Ollama!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is split into two versions, this version uses OpenAI - providing **S**tate **o**f **t** **A**rt performance via their API. If you'd prefer to run everything locally you can try the [Ollama version](https://github.com/aurelio-labs/agents-course/blob/main/04-langchain-ecosystem/01-langchain-essentials/01-langchain-intro-ollama.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to settup our imports to make sure we have both models we want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add llama here too.\n",
    "import openai\n",
    "\n",
    "# OpenAI key to use the model\n",
    "OPENAI_API_KEY = \"\"\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ollama pull llama3.2:1b-instruct-fp16* - Use in terminal to download correct model we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# Here we can list a couple of models to choose from.\n",
    "openai_model = \"gpt-4o-mini\"\n",
    "llama_model = \"llama3.2:1b-instruct-fp16\"\n",
    "\n",
    "# Choose between which model you would like to use\n",
    "active_model = openai_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the task at hand is to add any lyrics into the 'song' variable, then we want to create three core pieces from this song:\n",
    "\n",
    "1. Song title\n",
    "2. Song description\n",
    "3. One additional verse in the song.\n",
    "\n",
    "We will make all of this using a sequential chain, and then we will display it as:\n",
    "\n",
    "> Original song:\n",
    "\n",
    "> New song name:\n",
    "\n",
    "> New song description:\n",
    "\n",
    "> New verse:\n",
    "\n",
    "> Where the new verse should be played in the song:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can input our song to start us of with, currently this is using Viva La Vida by Coldplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = \"\"\"\n",
    "\\\n",
    "I used to rule the world \\\n",
    "Seas would rise when I gave the word \\\n",
    "Now in the morning, I sleep alone \\\n",
    "Sweep the streets I used to own \\\n",
    "I used to roll the dice \\\n",
    "Feel the fear in my enemy's eyes \\\n",
    "Listen as the crowd would sing \\\n",
    "Now the old king is dead, long live the king \\\n",
    "One minute, I held the key \\\n",
    "Next the walls were closed on me \\\n",
    "And I discovered that my castles stand \\\n",
    "Upon pillars of salt and pillars of sand \\\n",
    "\\\n",
    "I hear Jerusalem bells a-ringin' \\\n",
    "Roman Cavalry choirs are singin' \\\n",
    "Be my mirror, my sword and shield \\\n",
    "My missionaries in a foreign field \\\n",
    "For some reason, I can't explain \\\n",
    "Once you'd gone, there was never, never an honest word \\\n",
    "And that was when I ruled the world \\\n",
    "\\\n",
    "It was a wicked and wild wind \\\n",
    "Blew down the doors to let me in \\\n",
    "Shattered windows and the sound of drums \\\n",
    "People couldn't believe what I'd become \\\n",
    "Revolutionaries wait \\\n",
    "For my head on a silver plate \\\n",
    "Just a puppet on a lonely string \\\n",
    "Aw, who would ever wanna be king? \\\n",
    "\\\n",
    "I hear Jerusalem bells a-ringin' \\\n",
    "Roman Cavalry choirs are singing \\\n",
    "Be my mirror, my sword and shield \\\n",
    "My missionaries in a foreign field \\\n",
    "For some reason, I can't explain \\\n",
    "I know Saint Peter won't call my name \\\n",
    "Never an honest word \\\n",
    "But that was when I ruled the world \\\n",
    "\\\n",
    "Oh-oh-oh, oh-oh, oh \\\n",
    "Oh-oh-oh, oh-oh, oh \\\n",
    "Oh-oh-oh, oh-oh, oh \\\n",
    "Oh-oh-oh, oh-oh, oh \\\n",
    "Oh-oh-oh, oh-oh, oh \\\n",
    "\\\n",
    "I hear Jerusalem bells a-ringin' \\\n",
    "Roman Cavalry choirs are singin' \\\n",
    "Be my mirror, my sword and shield \\\n",
    "My missionaries in a foreign field \\\n",
    "For some reason I can't explain \\\n",
    "I know Saint Peter won't call my name \\\n",
    "Never an honest word \\\n",
    "But that was when I ruled the world \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to import all our libaries needed for using LangChain's sequential chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will make a LLM able to connect to our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'temperature': 0.0, 'ope...ne, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[839], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     creative_llm \u001b[38;5;241m=\u001b[39m ChatOllama(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, model\u001b[38;5;241m=\u001b[39mllama_model)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m active_model \u001b[38;5;241m==\u001b[39m openai_model:\n\u001b[1;32m----> 5\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenai_api_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mOPENAI_API_KEY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     creative_llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, model\u001b[38;5;241m=\u001b[39mopenai_model, openai_api_key \u001b[38;5;241m=\u001b[39m OPENAI_API_KEY)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Joshu\\OneDrive\\Documents\\Aurelio\\agents-course\\04-langchain-ecosystem\\01-langchain-essentials\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joshu\\OneDrive\\Documents\\Aurelio\\agents-course\\04-langchain-ecosystem\\01-langchain-essentials\\.venv\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Joshu\\OneDrive\\Documents\\Aurelio\\agents-course\\04-langchain-ecosystem\\01-langchain-essentials\\.venv\\Lib\\site-packages\\pydantic\\main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'temperature': 0.0, 'ope...ne, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/value_error"
     ]
    }
   ],
   "source": [
    "if active_model == llama_model:\n",
    "    llm = ChatOllama(temperature=0.0, model=llama_model)\n",
    "    creative_llm = ChatOllama(temperature=0.9, model=llama_model)\n",
    "elif active_model == openai_model:\n",
    "    llm = ChatOpenAI(temperature=0.0, model=openai_model, openai_api_key = OPENAI_API_KEY)\n",
    "    creative_llm = ChatOpenAI(temperature=0.9, model=openai_model, openai_api_key = OPENAI_API_KEY)\n",
    "else:\n",
    "    raise ValueError(\"You need to apply the correct name to the model you are referring to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get into our chain, for the sequential chains it is important to note everything is done within a string, and not an f string, and keep an eye on how variables are passed in and out, as if these get messed up it won't create an error here and instead create an error at the overall chain..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the system prompt (how the AI should act)\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI assistant that helps generate song titles.\"\n",
    ")\n",
    "\n",
    "# Defining the user prompt (what we want the AI to do)\n",
    "user_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"Can you make a title for this song:\"\n",
    "    \"\\n\\n{song}\"\n",
    ")\n",
    "\n",
    "# prompt template 1: making a song title\n",
    "first_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])\n",
    "\n",
    "# chain 1: inputs: song / outputs: song_title\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"song_title\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an AI assistant that helps generate song titles.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['song'], input_types={}, partial_variables={}, template='Can you make a title for this song:\\n\\n{song}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(first_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 2: making a song description\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following song in 2 sentences (max 40 words) using the song title in the summary:\"\n",
    "    \"\\n\\n{song}\\n\\n{song_title}\"\n",
    ")\n",
    "# chain 2: inputs: song, song_title / outputs: summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['song', 'song_title'], input_types={}, partial_variables={}, template='Can you summarize the following song in 2 sentences (max 40 words) using the song title in the summary:\\n\\n{song}\\n\\n{song_title}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(second_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: creating a new verse\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can make one short verse for this song:\\n\\n{song}\"\n",
    ")\n",
    "# chain 3: inputs: song / output: new_verse\n",
    "chain_three = LLMChain(llm=creative_llm, prompt=third_prompt,\n",
    "                       output_key=\"new_verse\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 4: adding the new verse to the original music\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"where should the {new_verse} be played in the song {song}, \\\n",
    "        explicitly say which line of the original song this new verse should come afterwards\"\n",
    ")\n",
    "# chain 3: inputs: song, new_verse / outputs: new_song\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                       output_key=\"new_song\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all our chains ready we can make a 'overall' chain, this will bind all our chains together and is critical for any of this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],                 #   all the previous chains we just made\n",
    "    input_variables=[\"song\"],                                               #   where our song will be inputted\n",
    "    output_variables=[\"song_title\", \"summary\",\"new_verse\",\"new_song\"],      #   all the outputs we create\n",
    "    verbose=True                                                            #   to show AI trail of thought\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all settup we can call our overall_chain like a function, passing in only one variable song!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'song': \"\\nI used to rule the world Seas would rise when I gave the word Now in the morning, I sleep alone Sweep the streets I used to own I used to roll the dice Feel the fear in my enemy's eyes Listen as the crowd would sing Now the old king is dead, long live the king One minute, I held the key Next the walls were closed on me And I discovered that my castles stand Upon pillars of salt and pillars of sand I hear Jerusalem bells a-ringin' Roman Cavalry choirs are singin' Be my mirror, my sword and shield My missionaries in a foreign field For some reason, I can't explain Once you'd gone, there was never, never an honest word And that was when I ruled the world It was a wicked and wild wind Blew down the doors to let me in Shattered windows and the sound of drums People couldn't believe what I'd become Revolutionaries wait For my head on a silver plate Just a puppet on a lonely string Aw, who would ever wanna be king? I hear Jerusalem bells a-ringin' Roman Cavalry choirs are singing Be my mirror, my sword and shield My missionaries in a foreign field For some reason, I can't explain I know Saint Peter won't call my name Never an honest word But that was when I ruled the world Oh-oh-oh, oh-oh, oh Oh-oh-oh, oh-oh, oh Oh-oh-oh, oh-oh, oh Oh-oh-oh, oh-oh, oh Oh-oh-oh, oh-oh, oh I hear Jerusalem bells a-ringin' Roman Cavalry choirs are singin' Be my mirror, my sword and shield My missionaries in a foreign field For some reason I can't explain I know Saint Peter won't call my name Never an honest word But that was when I ruled the world \",\n",
       " 'song_title': '\"Echoes of a Fallen Crown\"',\n",
       " 'summary': '\"Echoes of a Fallen Crown\" reflects on the rise and fall of power, where the narrator reminisces about their former glory and the emptiness that follows their loss. The song captures the fleeting nature of authority and the haunting memories of a once-dominant reign.',\n",
       " 'new_verse': 'In shadows of a throne long lost,  \\nEchoes of power, a heavy cost.  \\nOnce I stood tall, now I just roam,  \\nA king without a crown, far from home.  ',\n",
       " 'new_song': 'The new verse you provided should be placed after the line:\\n\\n\"And that was when I ruled the world.\"\\n\\nSo the full sequence would be:\\n\\n\"And that was when I ruled the world.  \\nIn shadows of a throne long lost,  \\nEchoes of power, a heavy cost.  \\nOnce I stood tall, now I just roam,  \\nA king without a crown, far from home.\"'}"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_chain(song)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
