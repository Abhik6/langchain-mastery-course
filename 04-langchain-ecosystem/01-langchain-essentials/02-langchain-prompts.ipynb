{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts templates for OpenAI & Llama - LangChain #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most fundamental and underrated pieces of langchain is prompting, prompts are what we would send to our LLM to allow them to answer in a formulated response, super useful if we want to do something such as making NPCs speak with a specific accent or creating a song with a specifc twist in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 2: Prompts**\n",
    "\n",
    "Once we have mastered prompts we will be able to dive deeper into chains, memory, and everything else!\n",
    "\n",
    "1. LangChain uses it's own `PromptTemplate` handler which we will use to feed our LLMs.\n",
    "2. Using prompts we can make both `system` prompts and `user` prompts.\n",
    "3. We will go over `few shot learning` and how that can help models perform better.\n",
    "4. We will also make a `limit` on how many examples a prompt can give to the LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'            \\nAnswer the question based on the context below,                 }\\nif you cannot answer the question using the                     }--->  (Rules) For Our Prompt\\nprovided information answer with \"I don\\'t know\"                 }\\n\\nContext: Aurelio AI is an AI development studio                 }\\nfocused on the fields of Natural Language Processing (NLP)      }\\nand information retrieval using modern tooling                  }--->   Context AI has\\nsuch as Large Language Models (LLMs),                           }\\nvector databases, and LangChain.                                }\\n\\nQuestion: Does Aurelio AI do anything related to LangChain?     }--->   User Question\\n\\nAnswer:                                                         }--->   AI Answer\\n'"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"            \n",
    "Answer the question based on the context below,                 }\n",
    "if you cannot answer the question using the                     }--->  (Rules) For Our Prompt\n",
    "provided information answer with \"I don't know\"                 }\n",
    "\n",
    "Context: Aurelio AI is an AI development studio                 }\n",
    "focused on the fields of Natural Language Processing (NLP)      }\n",
    "and information retrieval using modern tooling                  }--->   Context AI has\n",
    "such as Large Language Models (LLMs),                           }\n",
    "vector databases, and LangChain.                                }\n",
    "\n",
    "Question: Does Aurelio AI do anything related to LangChain?     }--->   User Question\n",
    "\n",
    "Answer:                                                         }--->   AI Answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to settup our imports to make sure we have both models we want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# OpenAI key to use the model\n",
    "OPENAI_API_KEY = \"\"\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ollama pull llama3.2:1b-instruct-fp16* - Use in terminal to download correct model we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# Here we can list a couple of models to choose from.\n",
    "openai_model = \"gpt-4o-mini\"\n",
    "llama_model = \"llama3.2:1b-instruct-fp16\"\n",
    "\n",
    "# Choose between which model you would like to use\n",
    "llm_model = openai_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the AI will appoach our question, as you can see we have a formulated response, if the context has the answer, then use the context to answer the question, if not, say I don't know, then we also have context and question which are being passed into this similarly to paramaters in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prompt = \"\"\"\n",
    "Answer the question based on the context below,                 \n",
    "if you cannot answer the question using the                   \n",
    "provided information answer with \"I don't know\"\n",
    "\n",
    "context: {context}\n",
    "\n",
    "question: {query}\n",
    "\"\"\"\n",
    "Context = \"\"\"\n",
    "Context: Aurelio AI is an AI development studio                 \n",
    "focused on the fields of Natural Language Processing (NLP)      \n",
    "and information retrieval using modern tooling                  \n",
    "such as Large Language Models (LLMs),                           \n",
    "vector databases, and LangChain. Owned by Andrew Tate  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we want to create our template we will use for our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Passing the template to the LangChain model.\n",
    "prompt_template = ChatPromptTemplate.from_template(Prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to open a LLM where we can ask our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply any question into here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = \"Who owns Aurelio AI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we're converting the message into the correct format for our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt_template.format_messages(prompt=Prompt, query=Question, context=Context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_model == llama_model:\n",
    "    llm = ChatOllama(temperature=0.0, model=llama_model)\n",
    "    creative_llm = ChatOllama(temperature=0.9, model=llama_model)\n",
    "elif llm_model == openai_model:\n",
    "    llm = ChatOpenAI(temperature=0.0, model=openai_model, openai_api_key = OPENAI_API_KEY)\n",
    "    creative_llm = ChatOpenAI(temperature=0.9, model=openai_model, openai_api_key = OPENAI_API_KEY)\n",
    "else:\n",
    "    raise ValueError(\"You need to apply the correct name to the model you are referring to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will retrieve our answer and showcase the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aurelio AI is owned by Andrew Tate.\n"
     ]
    }
   ],
   "source": [
    "answer = llm(messages)\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works for the basic chatbots, however what if we want to give some questions and answers so that the AI can have a rough idea of what we want it to say?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will cover Few Shot Prompt Templates allowing us to be able to go more in depth with our AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Firstly we want to add a example template, this will tell the AI how we would like to format our inputs and outputs, in this case, the questions and answers.\n",
    "2. Secondly we want to add a prompt template, this again will be used to tell the AI how we would like it to use the examples and answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the example template\n",
    "example_template = \"\"\"Input: {input}\n",
    "Output: {output}\"\"\"\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"Given the following examples, complete the task:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of questions and answers, here you can see that we're trying to match words based on their similarity to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples = [\n",
    "        {\"input\": \"Clouds, Sky, Sun, Space, Planets\", \"output\": \"Galaxy\"},\n",
    "        {\"input\": \"Eyes, Face, Human, Animal\", \"output\": \"Species\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to make a few shot template where we can feed all this data into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=Examples,\n",
    "    example_prompt=PromptTemplate.from_template(example_template),\n",
    "    prefix = prompt_template,\n",
    "    suffix=\"Input: {input}\\nOutput:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can carry on with our question to see what it thinks is similar too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following examples, complete the task:\n",
      "\n",
      "Input: Clouds, Sky, Sun, Space, Planets\n",
      "Output: Galaxy\n",
      "\n",
      "Input: Eyes, Face, Human, Animal\n",
      "Output: Species\n",
      "\n",
      "Input: Fitness, Health, Lifestyle\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt with input\n",
    "formatted_prompt = few_shot_prompt.format(input=\"Fitness, Health, Lifestyle\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to run this through our model and see what the output is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wellness\n"
     ]
    }
   ],
   "source": [
    "output = creative_llm(messages=[HumanMessage(content=formatted_prompt)])\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However that is considered the old way of doing few shot prompts, here below we can see a newer way which is way easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Clouds, Sky, Sun, Space, Planets\n",
      "AI: Galaxy\n",
      "Human: Eyes, Face, Human, Animal\n",
      "AI: Species\n"
     ]
    }
   ],
   "source": [
    "# This is a prompt template used to format each individual example.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "# This is our few shot template used to feed our examples into the LLM\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=Examples,\n",
    ")\n",
    "# Here we can view the format of our content down below\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a poem writer.'), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': 'Clouds, Sky, Sun, Space, Planets', 'output': 'Galaxy'}, {'input': 'Eyes, Face, Human, Animal', 'output': 'Species'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Here we are formatting our final prompt for the AI to use, take note that the input needs to be the same keyword as the input below.\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a poem writer.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "print(final_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Fitness, Health, Lifestyle', 'text': 'Wellness'}"
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(prompt=final_prompt, llm=creative_llm)\n",
    "\n",
    "chain.invoke({\"input\": \"Fitness, Health, Lifestyle\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to dive into Chain-Of-Thought prompting, for this there is no direct prompt template however instead what we do is we set the template up to make the AI talk to us about how it will solve the problem, by going through each step rather then just rushing straight to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will settup the step by step rule to enable chain of thought prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "First, list systematically and in detail all the problems in this question\n",
    "that need to be solved before we can arrive at the correct answer.\n",
    "Then, solve each sub problem using the answers of previous problems\n",
    "and reach a final solution\n",
    "\n",
    "Output: Well the right answer is... \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain-of-thought prompt template\n",
    "template2 = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Do not use any chain-of-thought processes to answer the question\n",
    "\n",
    "Output: Well the wrong answer is... \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the rest is the same as a simple prompt, where we feed a basic question into the LLM and it will give us a answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "prompt2 = PromptTemplate(template=template2, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the question-answering chain with the chain-of-thought prompt\n",
    "chain = LLMChain(prompt = prompt, llm = llm)\n",
    "# Load the question-answering chain with the chain-of-thought prompt\n",
    "chain2 = LLMChain(prompt = prompt2, llm = llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "question1 = \"How many keystrokes are needed to type the numbers from 1 to 500? Answer Choices:\" # 1392 is the correct answer\n",
    "question2 = \"James has 7 apples, he eats 4 and is given an additional 19 apples, James gives 15 apples to Josh, and Josh gives James 2 apples, how many apples does James have?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how many keystrokes are needed to type the numbers from 1 to 500, we need to break down the problem into several sub-problems. Here are the steps we need to take:\n",
      "\n",
      "### Sub-Problem 1: Count the Number of Digits in Each Range\n",
      "We need to categorize the numbers from 1 to 500 based on how many digits they have:\n",
      "\n",
      "1. **1-digit numbers (1 to 9)**: \n",
      "   - There are 9 numbers (1, 2, 3, 4, 5, 6, 7, 8, 9).\n",
      "   - Each of these numbers has 1 digit.\n",
      "\n",
      "2. **2-digit numbers (10 to 99)**: \n",
      "   - There are 90 numbers (10 to 99).\n",
      "   - Each of these numbers has 2 digits.\n",
      "\n",
      "3. **3-digit numbers (100 to 499)**: \n",
      "   - There are 400 numbers (100 to 499).\n",
      "   - Each of these numbers has 3 digits.\n",
      "\n",
      "4. **The number 500**: \n",
      "   - This is a single number with 3 digits.\n",
      "\n",
      "### Sub-Problem 2: Calculate the Total Keystrokes for Each Range\n",
      "Now we will calculate the total number of keystrokes for each range of numbers:\n",
      "\n",
      "1. **Total keystrokes for 1-digit numbers**:\n",
      "   - Number of 1-digit numbers = 9\n",
      "   - Total keystrokes = 9 * 1 = 9\n",
      "\n",
      "2. **Total keystrokes for 2-digit numbers**:\n",
      "   - Number of 2-digit numbers = 90\n",
      "   - Total keystrokes = 90 * 2 = 180\n",
      "\n",
      "3. **Total keystrokes for 3-digit numbers (100 to 499)**:\n",
      "   - Number of 3-digit numbers = 400\n",
      "   - Total keystrokes = 400 * 3 = 1200\n",
      "\n",
      "4. **Total keystrokes for the number 500**:\n",
      "   - Number of digits = 3\n",
      "   - Total keystrokes = 3\n",
      "\n",
      "### Sub-Problem 3: Sum All Keystrokes\n",
      "Now we will sum the total keystrokes from all ranges:\n",
      "\n",
      "- Total keystrokes = (Total for 1-digit numbers) + (Total for 2-digit numbers) + (Total for 3-digit numbers) + (Total for 500)\n",
      "- Total keystrokes = 9 + 180 + 1200 + 3\n",
      "\n",
      "### Final Calculation\n",
      "Now we perform the final addition:\n",
      "\n",
      "- Total keystrokes = 9 + 180 + 1200 + 3 = 1392\n",
      "\n",
      "### Conclusion\n",
      "Well, the right answer is **1392 keystrokes** needed to type the numbers from 1 to 500.\n"
     ]
    }
   ],
   "source": [
    "result = chain.run(question1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well the wrong answer is 1500.\n"
     ]
    }
   ],
   "source": [
    "result = chain2.run(question1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see with the amount of steps involved without using a step by step guide, the AI can step over important information that would help it achieve the correct results, however it's important to note that without a super long question the AI can solve alot of these issues without using chain prompting, however it's just in these certain instances where chain prompting can be very useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
